
We thank the reviewers for their work.

Our *main contribution* is the application of automatic amortized
resource analysis to system code written in C.  This is not a simple
extension of existing techniques.  For example, the extension to
signed integers was a difficult open problem for more than a decade.
It has been investigated by different groups (e.g., Samir Genaim and
Martin Hofmann) but has not been addressed in earlier work (including
FLOPS'14).  As with every open problem, it is tempting to say it's
simple after you have seen the solution.  In general, there are many
new challenges when moving to C such as break and return statements,
bounds that depend on mutable (global and local) variables (also not
in FLOPS'14), and the need of a logical state to keep track of
relations such as x<y.

*Another contribution* is the *combination* of the program logic and
the automatic analysis in the same framework.  We deal with an
undecidable problem; so in practice there will always be programs for
which the automation will fail (no matter how sophisticated it is).
We propose the first framework for bound derivation in which users can
seamlessly combine manually-derived and (non-trivial)
automatically-derived bounds to deal with such programs.  The manual
bound derivation is very flexible can of course depend on *data
structure sizes and pointer arithmetic*.

Our automatic tool for bound analysis (AAA) and all benchmarks are
*publically available*.  It can be found in the supplementary material
and can also be conveniently accessed in a user-friendly web interface
at

  http://www.cs.yale.edu/homes/qcar/aaa/

Our tool *scales well to larger programs*; LP solvers can solve
millions of constraints within a few seconds.  For example, the file
'MD5 Sum' from MiBench on the web site has 260 LOC and is analyzed in
less than 3 seconds on a usual laptop.  There are also many
possibilities to improve performance further.

There is really nothing magical going on in the implementation and the
*algorithm arises almost automatically from the presented rules*
without any fancy heuristics.  Please see our detailed response to
Review #31D, which we would also include in the accepted paper.

It is of course possible to *integrate abstract interpretation* with
our framework.  On a high level, this is described in the paper
"Automatic construction of Hoare proofs from abstract interpretation
results" (APLAS'03).  We are already using a simple form of abstract
interpretation to derive the logical state (\Gamma) that we use in the
automation.  It is possible to use more involved abstractions and
fixpoint computations.  The point that we were trying to make is that
we found it to be surprising that we can handle all these challenging
examples without using clever state of the art abstract
interpretations and fixpoint computations.

As reviewer #31A suggests, there are different ways to implement the
logic. However, the integration with Clight and the soundness proof in
Coq are never trivial.  There are also some details that make the
extension of Hoare logic non-trivial.  For instance, the rule L:Frame
is not derivable from the rules of classic Hoare logic but essential
in most derivations.

===

Review #31B:

> In the instrumented semantics, I'm not convinced that it is worth
> the effort to give possibly non-zero costs to basic operations of
> the language (expression evaluation, control structures, etc).

Not sure if it's worth the effort but it comes in handy at points.
For instance, we proved that bounds in the logic prove termination in
a metric in which all these constants are positive.  Alternatively, we
would need to have ticks at the right places which is a bit less
elegant to reason about.

> On a somewhat related note, precise WCET analysis tools like
> AbsInt's a-cube are able to derive very precise timings, provided
> they are supplied with tight bounds on the number of iterations of
> every loop in the program.  Could (a variant of) the quantitative
> analysis of section 5 infer such bounds?

Absolutely: Just insert a tick(1) into the loop for which you would
like to derive a bound.  Conversely, you can use AbsInt's tool to
derive bounds on straight-line code and insert 'ticks' into the
program to derive a parametric bound.  The open questions we are
working on are: How to integrate this into CompCert to prove (in Coq)
end-to-end guarantees on WCET?  How to handle cache and pipeline
effects *between* basic blocks?

> I'm curious to know what it would take for AAA to be able to infer
> nonlinear bounds.  Is this just a matter of replacing the LP solver
> by something more powerful, or is there something in the framework
> of section 5 that is specific to linear constraints?

We have developed a theory that makes it possible to infer *polynomial
bounds* with *linear* constraint solving only.  It is based on a set
of polynomials that we call 'multivariate resource polynomials'.  This
is not trivial (a bit like linear algebra) and was a quite surprising
result to people in resource analysis community.  Linear constraints
are important because we can solve millions of them efficiently in
practice.  (See for instance "Multivariate Amortized Resource
Analysis", POPL'11).

We designed the present analysis system with an extension to these
multivariate resource polynomials in mind.  However, this makes the
rules much more involved and is substantial additional implementation
effort.  So we will report on this in a separate paper.


Review #31C:

> 3) This is the direct incremental sequel of a PLDI'14 paper. The
> main increment is the i) treatment of symbolic resource bounds in
> the program logic ii) an automatic analysis tool that generates a
> proof tree in a had hoc proof system iii) the soundness of the had
> hoc proof system wrt to the quantitative hoard proof system. I'm
> unsure point i) makes a contribution in itself.

This work is not incremental.  For instance, we present an automatic
amortized resource analysis for C programs.  The PLDI'14 paper is only
about stack-usage bounds and has very limited support for automation.
The focus of the PLDI work was the integration of C-level stack-bounds
with verified compilation.

> 4) The current static analysis is rather limited. Not requiring a
> fixpoint is a disadvantage in my point of view, not a quality. It
> means the inference capability does not benefit from modern abstract
> interpretation techniques. Basically the lattice is flat. Being
> successful on benchmarks does not mean the analysis is better than
> others (like SPEED). I presume other approach are more robust, more
> generics than the current one.

As discussed at the beginning, abstract interpretation can of course
be integrated in our framework.  In fact, our automatic system relies
on an abstract program state that is inferred using a minimal form of
abstract interpretation (no complex fixpoints computations, narrowings
and widenings).  Our abstract domain (to use the abstract
interpretation terminology) is formed by conjunctions of linear
inequalities on program variables.  It forms a lattice that is *not
flat*, for instance, we have:

                           0 < 1
                             |
                             |
                           0 < 0

It is always good to use successful existing techniques such as
abstract interpretation and we would be happy to learn about objective
advantages that the approach of SPEED has. In the meantime, we can
name the following advantages of our work over SPEED:

1) Our implementation is publically available and we invite the
   reviewers to test it.  In contrast, we were not able to obtain the
   SPEED tool from its authors.

2) Our automatic analysis produces machine-checkable certificates.  In
   contrast, SPEED relies on complicated external tools such as
   abstract interpretation based invariant generation.

3) Our automatic analysis can be combined with manually-derived bounds.
   In contrast, SPEED has no support for user interaction.

4) Our automatic analysis is compositional and naturally takes size
   changes of variables into account; thus we derive global
   whole-program bounds.  In contrast, SPEED derives local loop bounds
   only.

5) Our automatic analysis can deal with resources such as memory that
   may be returned during program execution.  In contrast, SPEED can
   only derive bounds on the number of loop iterations.

> 5) The current analysis is limited and the framework is not flexible
> enough to easily handle new abstractions, for example to go behond
> difference-bounds matrices invariants.

We think that our framework is more flexible than existing ones.  A
manually derived bound can be any function that is definable in Coq
(see, e.g, Figure 1 and Figure 7).  The automation is tailored to work
well for system code as we have it for instance in our CertiKOS
operating system.  It is no problem to add more automation to derive
bounds of a different shape.  We are working already on an extension
to polynomial bounds using techniques developed, e.g., in
"Multivariate Amortized Resource Analysis" (POPL'11).


Review #31D:

> In the end, the paper presents a novel approach which holds promise,
> and I tentatively recommend a weak acceptance.  However, there are
> some major questions.  The most important is that I cannot see
> rigorously how the algorithm arises from the inference rules.

We answer all technical questions of the reviewer in a PDF file that
is available at the following address.  Among other things it includes
an example derivation with the automatic inference rules that
illustrates the different steps of the automation: (1) syntax-directed
application of the rules, (2) constraint generation, (3) constraint
solving with an LP solver.

  http://www.cs.yale.edu/homes/hoffmann/popl15_derivation.pdf

In a nutshell, there is really nothing magical going on and the
algorithm arises almost automatically.

> I don't fully understand what is the meaning of "inductively". From
> my guess, I believe this is a non-trivial problem, given the
> existence of loops and the Q:WEAK rule.

All rules except Q:Weak are syntax-directed and we apply them
inductively, following the syntax tree.  The rule Q:WEAK can be simply
applied after each syntax-driven rule (the LP solver decides later if
it is actually used).  We however discovered, that it is sufficient to
apply it right before assignments and in the branches of if
statements.

When we apply the rules we instantiate the rational coefficients
(given by Q = (q_i) in the rules) with variables that are later
instantiated to rational numbers by the LP solver.  Loops are simply
handled by adding equality constraints to the linear program.  (See
the example in the PDF.)

> Also, the compositionality seems to be lost in the two phases
> (collecting the constraints and solving them). While in the
> evaluation, the running time for an individual example is
> negligible, I don't think the argument will carry to practical
> programs.

LP solvers are extremely efficient and can handle millions of
constraints in a few seconds.  Moreover, our constraints are
particularly simple and constitute so-called Network Problems that can
be solved in linear time in practice.  As a result, our implementation
scales to large programs.  To demonstrate that, we added the example
"MD5 Sum" from the MiBench benchmark suit to the online version of the
tool. It has 260 lines of code and can be analyzed in less than 3
seconds.

> Furthermore, I expect that often, when the solver successfully finds
> some solutions, there should be many (could be infinitely)
> solutions. How is the extraction performed in order to achieve
> precise bounds?

The LP solver is provided with a so-called objective function that it
minimizes.  The objective function is basically the sum of the
coefficients that appear in the precondition of the outermost Hoare
triple.  If this precondition is {p + q|[0,x]| + q'|[x,0]|} then we
would ask the solver to minimize p + q + q'.  LP solvers also support
a nice trick that lets us express that linear coefficients should be
minimized with higher priority than the constant coefficient p: We
would first ask the solver to minimize q + q' and obtain a solution S.
We then add the constraint q+q'=S(q)+S(q') to the set of constraints
and ask to minimize p.  This adds almost no computing time since
solvers have clever support for this.  (Also compare the example in
the PDF.)

> The paper hints out that going beyond linear bounds is possible.
> Given what is discussed in Section 5, I couldn't see any plausible
> connection.

Please see our answer at the very end of the response to Review #31B.

> Thus a rebuttal question is: can the authors present two small
> examples, one to demonstrate in details how the technique works. The
> other is to demonstrate when the technique does not work (e.g. when
> the loop pattern is too tricky).

The first example can be found in the previously mentioned PDF file.
It is to complex to present the derivation tree here.  The counter
example requires simply a bound of the form max(x+y-z,0) that we
cannot express in our current format:

  while (x+y < z) {
    if (nondet())
      x++;
    else
      y++;
    tick(1);
  }

The problem here is that we don't know that x<z (or y<z) when we
increment x (or y, respectively).  As a result, we don't have any
interval such as [x,z] that becomes smaller in the loop body.
However, if we assert that x and y are non-negative then we can derive
a bound:

  assert (x>=0);
  assert (y>=0);
  while (x+y < z) {
    if (nondet())
      x++;
    else
      y++;
    tick(1);
  }

  Computed bound: |[x, z]| + |[y, z]|

> And I believe that being a black-box and fully automated tool has
> its own advantages.

You can use our tool as a black box.  The only difference is that you
can provide manual proofs if the tool fails.  With the black-box
tools, you simply don't have this option.

> "t08a ... constant 0 and 1 ...". I have no idea which
> constants you are referring to.

We meant the constants that are integer literals in the C program that
is analyzed.  The constant 0 appears in line 1.  The constant 1
appears in line 3.

> From my understanding, in t15, "x -= y+1" is broken down into "x = x
> - 1" and "x = x - y".  My follow-up question: does the order between
> the two matter?  In other words, will the other order give the same
> bound?

The order is irrelevant for the bound.
