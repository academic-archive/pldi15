% Some notes:
%  1) Use Appel's SHA-256 implementation as an example
%

\documentclass[nocopyrightspace,preprint]{sigplanconf}

\usepackage[numbers]{natbib}

\usepackage[utf8]{inputenc} %for utf8 input
\usepackage{amssymb} %for shift symbol
\usepackage{amsmath}
\usepackage{listings} %for code
\usepackage{mathpartir} %for typing rules
\usepackage{microtype} %better micro typing
\usepackage{stmaryrd} %for llbracket
\usepackage{mathabx} % for boxes
\usepackage{graphicx} %to include png images
\usepackage{xcolor} %for colors
\usepackage{url}
\usepackage{enumitem}

%----------------------------------------------------

\usepackage{prettyref}
\newcommand{\pref}[1]{\prettyref{#1}}
\newcommand{\Pref}[1]{\prettyref{#1} \vpageref[]{#1}}
\newcommand{\ppref}[1]{\vpageref[]{#1}}
\newrefformat{fig}{Figure~\ref{#1}}
\newrefformat{app}{Appendix~\ref{#1}}
\newrefformat{tab}{Table~\ref{#1}}
\newrefformat{cha}{Challenge~\ref{#1}}
\newrefformat{compiler}{Point~\ref{#1} of \pref{thm:compiler}}

%----------------------------------------------------

\usepackage{amsthm}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}

%----------------------------------------------------

\input{definitions}

\begin{document}

\conferenceinfo{POPL} {January XX-XX, 2014, CITY, STATE, India.}
\copyrightyear{2014}
\copyrightdata{XXX-X-XXXX-XXXX-X/XX/XX}

%----------------------------------------------------


\titlebanner{Draft -- Do not distribute}        % These are ignored unless
\preprintfooter{Draft}   % 'preprint' option specified.

% \title{Automatic Amortized Analysis for Imperative Code}
\title{Compositional Quantitative Resource Analysis of C Programs}
% Compositional Resource-Bound Certification for C Programs

\authorinfo
{Quentin Carbonneaux \and Jan Hoffmann \and Zhong Shao}
{Yale University}


\maketitle


\begin{abstract}

\end{abstract}

\category{D.2.4}{Software Engineering}
{Software/Program Verification}
\category{F.3.1}{Logics and Meanings of Programs}
{Specifying and Verifying and Reasoning about Programs}

\terms Verification, Reliability


\keywords Formal Verification, Compiler Construction, Program Logics,
Stack-Space Bounds, Quantitative Verification

\section{Introduction}
\label{sec:intro}

In software engineering and software verification, we often would like
to have static information about the quantitative behavior of
programs.  For example, stack and heap-space bounds are important to
ensure the reliability of safety-critical systems [XXX (see PLDI)].
% Clock-cycle bounds are needed to guarantee the safety of real-time
% systems [XXX wilhelm].  
Static energy usage information is critical for autonomous systems and
has applications in cloud computing [XXX?  (cite this guy form the
us))].  Worst-case resource bounds can be used to prevent side-channel
attacks through resource padding [XXX?].  Loop and
recursion-depth bounds are used to ensure the accuracy of programs
that are executed on unreliable hardware [XXX oopsla] and complexity
bounds are needed to verify security protocols [XXX].
% XXX Mention differential privacy here?
%
In general, quantitative performance information at design time can
provide useful feedback for developers.

Static analysis of quantitative properties of imperative programs is
an active research area and recent years have seen many innovations.
Notable tools that have been developed include Speed [XXX], KoAT
[XXX], Pubs [XXX], RANK [XXX], and LOOPUS [XXX].
%
While the these tools can derive impressive results for realistic
software, there still exist shortcomings that hamper the application
of the existing techniques in practice.
\begin{enumerate}
\item Analysis tools are black boxes that either deliver a result or fail
  without enabling \emph{user interaction} to manually or
  semi-automatically derive bounds for challenging parts of the
  program.
\item The computed bounds are often \emph{non-compositional} local
  bounds (for a single (nested) loop) that are difficult to combine to
  global whole program bounds.
\item Existing techniques often rely on complex external tools such as
  abstract interpretation based invariant generation [XXX speed] or
  translation of the program into a term-rewriting system [costa,
  tacas] without providing \emph{verifiable certificates} for the
  correctness of the derived bound.
\end{enumerate}
%
While there has been lot of progress in static quantitative analysis,
Don Knuth correctly points out in a recent interview [XXX] that the
state-of-the-art in formal quantitative methods still falls short
in comparison with semantic techniques [xxx].
%
\begin{quote}
  % Consider, for example, a computer program that sorts a list of
  % numbers into order. 
  Thanks to the work of Floyd, Hoare, and others,
  we have formal definitions of semantics, and tools by which we can
  verify that sorting is indeed always achieved. My job is to go
  beyond correctness, to an analysis of such things as the program's
  running time $[\cdots]$. I'm 100\% sure that my recurrence correctly
  describes the program's performance, and all of my colleagues agree
  with me that the recurrence is "obviously" valid. Yet I have no
  formal tools by which I can prove that my recurrence is right. I
  don't really understand my reasoning processes at all!
  \vspace{-3ex}
  \begin{flushright}
    -- Donald Knuth, 2014
  \end{flushright}
\end{quote}
%
%
In this paper, we present a novel cost analysis system for C programs
that works automatically, efficiently and precisely for many examples,
can be used interactively if the automation fails, and produces
verifiable certificates for the bounds.  Our contributions are as
follows.
%
\begin{itemize}
\item We define a operational cost semantics for C programs that
  accounts a constant cost for each statement which is parametric in a
  cost metric.  The semantics models cost of terminating and
  non-terminating executions.  A specific \emph{tick} function can be
  inserted in the source code to further customize the cost
  model.
\item We describe an automatic static analysis technique for deriving
  bounds for the cost that is defined by the operational semantics.
  It is based on amortized resource analysis for functional programs
  [XXX] and derives bounds that are non-negative linear combinations
  of sizes $|[x,y]|$ of intervals between program variables $x,y$.
\item We show with a publically available prototype implementation and
  an experimental evaluation that our novel amortized resource
  analysis works efficiently for realistic programs and that the
  derived constant factors in the bounds are close to or identical
  with the optimal ones.
\item We define a quantitative Hoare logic for interactively deriving
  cost bounds for C programs.  The logic is implemented and proved
  sound in the Coq Proof Assistant with respect to CompCert C
  light.  We show that our automatic analysis readily produces
  derivations in the quantitative Hoare logic.
% \item We prove the completeness of the program logic
%   with respect to the operational cost semantics.
\end{itemize}
%
Our approach complements existing work since it provides a semantic
foundation for the computation of bounds while still providing support
for automation that sometimes goes beyond the capabilities of existing
techniques.  The quantitative Hoare logic and the automatic amortized
analysis enable semi-automatic interactive resource bound development
in which an automatically derived bound can be used in the
quantitative logic and a hand derived bound can be used by the
automatic analysis.  Both the automatic amortized analysis and the
quantitative logic are naturally compositional and describe the
resource behavior of code fragments without referring to the source
code.  As a result, we naturally derive global bounds that are
functions of the input parameters of the program.  To the best of our
knowledge, our system is the only one that can deal with resource like
memory that may become available during the execution.

The main innovation that makes this work practical is our new
automatic amortized analysis for C code.  It targets programs that are
common in system software, that is, programs whose resource
consumption can be described as a function of sizes $|[x,y]|$ of
intervals of integer variables.  A typical bound would be $B(x,y,z) =
10{\cdot}|[x,y]| + 1{\cdot}|[z,0]|$ (for example $B(100,200,43) =
1043$).
%
Despite the apparent simplicity of the new analysis system, it is able
to reproduce results from the literature which were obtained using
methods based on abstract interpretation.  In contrast with abstract
interpretation--based methods, our technique does not require any
fixpoint computations on an abstract domain to obtain loop invariants.
The mechanism we designed is able to leverage local assertions such as
$x < y$ that we collect along the branching points of the program to
obtain global invariants.  We achieve this by generating a linear
constraint system that reflects resource cost and size changes of
integer variables in the program.  A solution of the linear program is
a resource usage bound for the C program.  We can naturally, handle
advanced control flow such as break, return, iteration over negative
integer variables, and mutually-recursive functions.

Following the development steps of automatic amortized analysis for
functional programs [XXX], we deliberately restrict our self to linear
bounds in the automatic analysis (there a no restrictions for the
manually-derived bounds in the logic).  More specifically, bounds have
the form $\sum_{a,b} q_{(a,b)} |[a,b]|$ where $a$ and $b$ are integer
variables or constants.  The reason is mainly clarity of presentation,
since an extension to multivariate resource polynomials [XXX] would
make the inference rules considerably more involved and should
better be described separately.  However, we developed the linear
inference system so that the extension to polynomial bounds shall
work smoothly.  

We implemented our analysis for CompCert Clight, a subset of C in
which loops can only be exited with a \code{break} statement and
expressions are free of side effects.  C programs can for instance be
compiled to Clight programs using the verified CompCert C compiler.

Our new quantitative Hoare logic for CompCert Clight is a
generalization of a program logic that we recently developed to
formally very stack-space bounds in Coq [PLDI].  In a nutshell, we
replace the boolean predicates (pre and post conditions) of Hoare
logic with numeric predicates that map a program state to a
non-negative number or infinity.  The intuitive meaning of a
quantitative Hoare triple $\htriple{P}{C}{Q}$ that $C$ consumes $P(S)$
resources when executed in starting state $S$, and that $Q(S')$
resources are left if $C$ terminates in state $S'$.  Infinity
corresponds to \emph{false} in classic Hoare logic.  Our quantitative
logic is proved sound with respect to a cost semantic for Clight that
accounts a constant cost for each evaluation step.  This constant cost
is parametric in a resource metric that can be defined by the user.
Our shallow embedding in Coq enables the derivation of both, bounds
that are parametric in the cost metric or bounds for one specific
metric.

We have evaluated the quantitative logic and the automatic analysis
with system code and examples from the literature. \pref{app:cat}
contains a $50$ (XXX) challenging loop and recursion patterns that we
collected from open source software and the literature.  Our analysis
can find asymptotically tight bounds for $48$ of these patterns and in
most cases the derived constant factors are tight.  We compared our
automatic analyzer to other resource-analysis systems.  However, our
analyzer and Rank are currently the only publically available tools
that can directly operate on C programs.  We selected $5$ example for
which we derived linear bounds and translated them to term-rewriting
systems by hand in order to run them with the tools KoAT and Pubs.
Both tools and Rank were not able to find linear bounds for the
examples.  We contacted the authors of LOOPUS and SPEED but were not
able to obtain a tool to perform a comparison.

To show the practicability of our approach we developed a case study
with different real-time schedulers implemented in C.  To perform a
schedulability analysis that takes into account the overhead of the
scheduler we derived time bounds for context switching and
initialization of the scheduler.  We then computed time bounds for a
set of given user task and took account the previously computed
scheduler overhead to determine whether the scheduling requirements of
all user tasks can be met.

With the intention, of making the material easily accessible we first
informally describe the new automatic amortized analysis
(\pref{sec:inform}).  We then formalize the intuition with the
operational cost semantics an the inference rules for the amortized
analysis (\pref{sec:aa}).  In \pref{sec:logic} we describe the
quantitative Hoare logic and its soundness proof that is formalized in
Coq.  In \pref{sec:inter} we prove the soundness of the automatic
amortized analysis by showing that derivation using the inference
rules can be seen as derivations in the quantitative logic.  We also
show how to integrate hand-derived bounds with automatically-derived
bounds.  \pref{sec:exper} contains the results of the experimental
evaluation and \pref{sec:related} describes related research.


\section{Informal Account}
\label{sec:inform}

In this section, we informally introduce the quantiatative program
logic and the automatic amortized analysis for for Clight programs.
The purpose of this section is to build some intuition that should
make the following sections, which contain more details, easy to read.

\subsection{Quantitative Hoare Logic}

The idea that drives the design of our system is amortized analysis
[XXX].  Assume that a program $p$ executes on a stating state
$\state_0$ and consumes $n$ resource units of some quantity we are
interested in.  We denote that by writing $(p, \state_0)
\Downarrow_n$.  The basic idea of amortized analysis is to define a
potential function $\Phi$ that maps program states to non-negative
numbers and to show that $\Phi(\state) \geq n$ if $\state$ is a
program state such that such that $(p, \state) \Downarrow_n$.

Since programs can be composed we must enrich the evaluation judgement
$(p, \state) \Downarrow_{n}$ to talk about the state resulting from
the program execution.  We write $(p, \state) \Downarrow_n \state'$ to
say that $\state'$ is the program state after the execution of $p$ in
starting state $\sigma$.  We now use two potential functions, one that
applies before the execution, and one that applies after.  The two
functions must respect the relation $\Phi(\state) \ge n +
\Phi'(\state')$ for all states $\state$ and $\state'$ such that $(p,
\state) \Downarrow_n \state'$.  Intuitively, $\Phi(\state)$ must
provide enough \emph{potential} for, both, paying for resource cost of
the computation and paying for the potential $\Phi'(\state')$ on the
resulting state $\state'$. That way, if $(\state, p_1) \Downarrow_n
\state'$ and $(\state', p_2) \Downarrow_m \state''$, we get
$\Phi(\state) \ge n + \Phi'(\state')$ and $\Phi'(\state') \ge m +
\Phi''(\state'')$.  This can be composed to $\Phi(\state) \ge (n + m)
+ \Phi''(\state'')$.  Note that the initial potential function $\Phi$
provides an upper bound on the resource consumption of the whole
program.  What we have observed is that, if we define $\htriple{\Phi}
{p}{\Phi'}$ to mean
$$
\forall \state, n, \state' : (\state, p) \Downarrow_n \state' \implies \Phi(\state) \ge
n + \Phi'(\state')
$$
then we get the following familiar looking rule.
$$
\RuleNolabel
{\htriple{\Phi}{p_1}{\Phi'} \\ \htriple{\Phi'}{p_2}{\Phi''}}
{\htriple{\Phi} {p_1; p_2}  {\Phi''}}
$$
%
Similarly, other language constructs lead to rules for the potential
functions that look very similar to Hoare logic or effect systems
rules.  This rules enable to interactively reason about resource usage
in a flexible and compositional way, which, as a side effect, produces
a certificate for the derived resource bound.

We have implemented and proved sound such a quantitative Hoare for
CompCert Clight in the Coq Proof Assistant.  Because we use a shallow
embedding in Coq, every function that can be expressed in Coq can in
principle be used as a resource bound.  We can use the logic to prove
a concrete bound for a given resource metric, or a bound that is
parametric in set of resource metrics.  It is also possible to
incorporate boolean preconditions into the bounds to express that the
bound is only valid for a certain class of inputs.  To this end, we
allow the potential function $\Phi$ in the pre and post conditions to
take non-negative number or $\infty$ (infinity) as values.  Infinity,
plays the same role as $\bot$ in Hoare logic.  Boolean assertions can
be embedded into potential functions by mapping $\bot$ to $\infty$ and
$\top$ to $0$.

It is for instance possible to derive the following quantitative
triple that holds for all stack metrics $M$.  A stack metric assigns
cost $0$ to all evaluation steps except function calls.  Before a call
$f(x)$, we consume $M(f) > 0$ resources and after the call we return
$-M(f)$ resources.  So $M(f)$ corresponds to the stack-frame size of
the function $f$.
$$
XXX
$$
XXX explanation of triple XXX

We say that the analysis is amortized because the use of a potential
function $\Phi$ gives the flexibility to amortize some operations.
For example, if program states are simple mappings from variables to
natural numbers, we have the following judgement (provided that
initially $\state(\code x) \ge \state(\code y) \ge 0$).
$$
\htriple
{\lambda \state . \, 2 {\cdot} \state (\code x) + 10}
{\code{ x = x -  y}}
{\lambda \state . \, 2 {\cdot} \state(\code x) + 2 {\cdot} \state(\code y) + 10}
$$
This must be understood as a motion of potential from the
variable~$\code x$ to the variable~$\code y$. If some resource is
consumed $\code y$ times afterwards, we can then use $\code y$'s
potential to pay for it. However, the initial potential function
$\Phi$ only refers to the variable~$\code x$: the potential function
allows us to pass on the cost of the subsequent resource consumption
to~$\code x$.  This flexibility combined with intelligent decisions on
potential motions allows us to design an automatic analysis to
discover linear invariants for surprisingly convoluted programs.

\subsection{Automatic Amortized Analysis}

A program logic provides a principled foundation for statically
analyzing programs.  However, program logics need to be supported by
automatic methods to be useful practice.  That is why we developed a
new automatic amortized analysis for deriving quantitative Hoare
triples for common C programs.

To this end, we fix the shape of the potential functions $\Phi$ so
that it becomes possible to use \emph{linear programming} (LP) to
compute a derivation in the logic.  If we assume that a program state
$\state$ simply maps variable to integers then we require
$$
\Phi(\state) = q_0 + \sum_{x,y \in \dom{\state}} q_{(x,y)} {\cdot} |[\state(x),\state(y)]|
$$
where $q_{(x,y)} \in \Qplus$ and $|[a,b]| = \max(b-a,0)$.  Note that
$q_0$ is the constant potential and that we treat constants as program
variables.  For instance, we always have $q_{(0,x)}|[0,x]|$ as a
component of $\Phi(\state)$ if $x \in \dom{\state}$.

We then develop an inference rule for each syntactic construct that
derives sound triples $\htriple{\Phi}p{\Phi'}$ in the quantitative
logic but enables inference using and LP solver.  This idea is best
explained by example.  Assume for the remainder of this section that
we are interested in bounding the number of \emph{ticks} in a program.
In other words, we use a resource metric that assigns cost $n$ to the
function call \code{tick(n)} and cost $0$ to all other operations.
The following example can then be bounded by $|[x,y]|$.
\begin{lstlisting}
  while (x<y) {x=x+1; tick(1);}
\end{lstlisting}
To derive this bound in our automatic amortized analysis, we start
with the initial potential $\Phi_0 = |[x,y]|$ (short for
$\Phi_0(\state) = |[\state(x),\state(y)]|$) which we also use as the
loop invariant.  For the loop body we then (like in Hoare logic) have
to derive a triple like
$\htriple{\Phi_0}{\code{x=x+1;tick(1)}}{\Phi_0}$.  However, we can
only do so if we exploit the fact that $x<y$ at the beginning of the
loop body.  The reasoning then works as follows.  We start with the
potential $|[x,y]|$ and the fact that $|[x,y]| > 0$ before the
assignment.  If we denote the update version of $x$ after the
assignment by $x'$ then the relation $|[x,y]| = |[x',y]| + 1$ between
the potential before and after the assignment \code{x=x+1} holds.
This means that we have the potential $|[x,y]| + 1$ before the
statement $tick(1)$.  Since \code{tick(1)} consumes one resource unit,
we end up with potential $|[x,y]|$ after the loop body and have
established the loop invariant again.

More formally, we develop (in section XXX) a judgement 
$$
\Gamma; Q \vdash S \dashv Q'; \Gamma'
$$
such that $\Gamma$ contains logical assertions like $x<y$ that we
collect along the conditional branches of the program and $Q$ is a
family of coefficents $Q = (q_{(a,b)})_{a,b \in \text{scope}}$ that is
indexed by the variables that are currently in scope.  The logical
assertions in $\Gamma$ are of the form XXXX.  They are only used to
determine at the assignment if we can extract constant potential to
pay for future cost.  It is important to note that the reasoning the
assertions in $\Gamma$ are very basic.  We do not perform any fixpoint
computations or invariants.

Note that our notion of a potential function is a refinement of the
concept of a ranking function.  A potential function can be used like
a ranking function if we use the tick metric and add \code{tick(1)} to
every back edge (loops and function calls) of the program.  However, a
potential function is more flexible.  For example, we can use a
potential function to prove that the following program does not
consume any resources in the tick metric.
\begin{lstlisting}
  while (x<y) {tick(-1); x=x+1; tick(1);}
\end{lstlisting}
Similarly we can prove that the following program can be bounded by
$10|[x,y]|$.  In both cases, we reason exactly like in the first
version of the while loop to prove the bound.  Of course, such loops
with different notations can be seamlessly combined in a larger
program.
\begin{lstlisting}
  while (x<y) {x=x+1; tick(10);}
\end{lstlisting}

To understand how the analysis works, it is instructive to look at some
more involved examples.  

\section{Syntax}

Let us now start the formal part of this work.  We begin by defining
a proper syntax for the language we will be using later on.  This is
a more algebraic version of the surface syntax used in the previous
examples.
%
\begin{mathpar}

S := \code{skip}
\mid \code{assert}(C)
\mid x \gets E
\mid \code{while}(C)~S
\mid \code{if}(C)~S~\code{else}~S
\mid S;S

\and

E := x \mid x \pm y
\end{mathpar}

Note that expressions are kept extremely simple, this will help
greatly when designing the analysis.  Moreover, programs
containing fancier expressions can easily be compiled to this
restricted form.  For example $a \gets b + c - d$ is semantically
equivalent to $a \gets b; a \gets a + c; a \gets a - d$ which
fits in the above syntax.

The syntax of conditions, noted $C$, is left unspecified
for the moment.  We will describe it later when the practical
implementation details are provided.  The only semantic tool
we need to deal with this unknown is an evaluation
function $\llbracket \_ \rrbracket_{\_}$ such that, for any
condition $C$ and any heap $H$ we have $\llbracket
C \rrbracket_H \in \{ \top, \bot \}$.

Another peculiarity is that there are no integer constants in the
syntax.  Once again this is for the sake of simplicity.  In practice
the concrete syntax recognizes ``variables'' like {\tt -4} or
{\tt 60} as read only variables.  Any heap used later in the
operational semantics will map these variables to their natural
value.

\section{Cost Semantics}

The semantics for the syntax defined above needs to account
for resource consumption.  To account for all kinds of resources
we leave it as a parameter of the semantics to be specified
by the user.  This parameter, called a metric, is a tuple of rational
numbers with the following form
$$
(
M_k, M_s, M_a,
M_w^1, M_w^2, M_w^3,
M_c^1, M_c^2,
M_s^1, M_s^2
).
$$
Each of these rational numbers indicates the amount of resource
consumed by an operation.  If one of these numbers is negative,
it means that some resource is released by the program.

The general form of an evaluation judgement is
$(H, S) \Downarrow_{(q,q')} X$.  It is similar to the judgement
presented in the overview section appart  from the fact that
two resource numbers are tracked instead of one.  Intuitively
$(q, q')$ means that $q$ resources units are necessary to execute
the program and that $q'$ units are available after its execution.
For example, $\code{alloc}(3)$ consumes 3 heap cells, so
the pair resulting from its evaluation would be $(3, 0)$; for
$\code{free}(3)$, the pair would be $(0, 3)$.  When these
statements are sequenced with \code{alloc} first, then \code{free},
the evaluation yields $(3, 3)$.  These resource pairs can
be composed with the following operation
$$
(q_1, q'_1) \cdot (q_2, q'_2) =
\left\{
\begin{array}{ll}
(q_1, q'_2 + q'_1 - q_2) & \mbox{if } q'_1 \ge q_2 \\
(q_1 + q_2 - q'_1, q'_2) & \mbox{if } q'_1 < q_2
\end{array}
\right.
.
$$
This operation can be seen as the sequential composition of
two resource consumptions, note that it is not a commutative
operation, the same way resource freeing and allocation do not
commute.
With $(0, 0)$ as a neutral element, the above composition defines
a monoid structure on $\mathbb Q_0^+ \times \mathbb Q_0^+$.
It is possible to inject $q \in \mathbb Q$ in this monoid with by taking
$(q, 0)$ if $q \ge 0$ and $(0, -q)$ otherwise.  This injection
is used implicitely in the semantics.

\begin{figure}[ht!]
\begin{mathpar}

\Rule{S:Abort}
{ }
{ (H, S) \Downarrow_{(0,0)} \circ }

\and \Rule{S:Skip}
{ }
{ (H, \code{skip}) \Downarrow_{M_k} H }

\and \Rule{S:Assign}
{ }
{ (H, x \gets e) \Downarrow_{M_s} H[x \mapsto \llbracket e \rrbracket_H] }

\and \Rule{S:Assert}
{ H \models C }
{ (H, \code{assert}(C)) \Downarrow_{M_a} H }

\and \Rule{S:WhileStop}
{ \llbracket C \rrbracket_H = \bot }
{ (H, \code{while}(C)~S) \Downarrow_{M_w^3} H }

\and \Rule{S:WhileAbort}
{  k < \llbracket x - y \rrbracket_H
\\ (H, S) \Downarrow_{(q_1, q'_1)} \circ
}
{ (H, \code{while}(k < x - y)~S) \Downarrow_{M_w^1 \cdot (q_1, q'_1) } \circ }

\and \Rule{S:WhileStep}
{  \llbracket C \rrbracket_H = \top
\\ (H, S) \Downarrow_{(q_1, q'_1)} H'
\\ (H', \code{while}(C)~S) \Downarrow_{(q_2, q'_2)} X
}
{ (H, \code{while}(C)~S) \Downarrow_{M_w^1 \cdot (q_1, q'_1) \cdot M_w^2 \cdot (q_2, q'_2)} X }

\and \Rule{S:IfThen}
{  (H, S_1) \Downarrow_{(q, q')} X
\\ \llbracket C \rrbracket_H = \top
}
{ (H, \code{if}(C)~S_1~\code{else}~S_2) \Downarrow_{M_c^1 \cdot (q, q')} X }

\and \Rule{S:IfElse}
{  (H, S_2) \Downarrow_{(q, q')} X
\\ \llbracket C \rrbracket_H = \bot
}
{ (H, \code{if}(C)~S_1~\code{else}~S_2) \Downarrow_{M_c^2 \cdot (q, q')} X }

\and \Rule{S:Seq}
{  (H, S_1) \Downarrow_{(q_1, q'_1)} H'
\\ (H', S_2) \Downarrow_{(q_2, q'_2)} X
}
{ (H, S_1; S_2) \Downarrow_{M_s^1 \cdot (q_1, q'_1) \cdot M_s^2 \cdot (q_2, q'_2)} X }

\and \Rule{S:SeqAbort}
{ (H, S_1) \Downarrow_{(q,q')} \circ }
{ (H, S_1; S_2) \Downarrow_{(q,q')} \circ }

\end{mathpar}
\caption{Operational semantics of statements}
\label{fig:opsem}
\end{figure}

Heaps manipulated by the operational semantics judgements are
simple maps from variable names to integers.  We designate by
$H[x \mapsto v]$ the heap $H$ updated to map the variable name
$x$ to the number $v$.
%
The evaluation of expressions is trivial using heaps.
\begin{mathpar}
\llbracket x \rrbracket_H = H(x)
\and
\llbracket x \pm y \rrbracket_H = H(x) \pm H(y)
\end{mathpar}

In the evaluation judgement $(H, S) \Downarrow_{(q, q')} X$, the result $X$ can
be either a new heap, which means the execution terminated successfully
or it can be the \emph{busy} result $\circ$.  This indicates an early
termination, it can be triggered at any point using the rule {\sc S:Abort}
and bubbles up through the execution context.
Sometimes there is no other choice than to use this latter rule, for
example when an assertion fails.  In that case the side condition of
{\sc S:Assert} is not met and the only rule that can possibly be applied
is {\sc S:Abort}.

The busy result is also useful to handle diverging programs with big-step
judgements.  In case a program does not terminate, there is no heap $H'$
such that $(H, S)\Downarrow_{(q, q')} H'$.  The only possible way to
conclude a big-step judgement is to use the {\sc S:Abort} rule at some
point in the computation.  One alternative to this technique is to use
coinductive semantics~\cite{Leroy-coind}, however, using the busy
result is precise enough in our setting of quantitative resource analysis and
allows to have only one soundness proof for both unsafe and
non-terminating programs.  Indeed, stating a result for
$(H, S) \Downarrow_{(q,q')} \circ$ covers unsafe programs, diverging
programs, and partial computations at the same time.


\section{Proof System}

We define in this section an abstract version of the proof system that
we will use to derive bounds in our practical implementation.

\subsection{Linear Potential Functions}

To get a practical and automated implementation of amortized analysis
as presented in the overview section we need the fix the shape of
potential functions.  The general form of a potential function is as follows
$$
\Phi(H) = q_0 + \sum_{a \prec b} q_{ab}\,|H(a) - H(b)|.
$$
Where $a$ and $b$ range over the set of all program variables $V$ and
$(q_{ab})$ and $q_0$ are non-negative rational numbers.  The relation
$\prec$ is an arbitrary complete order on $V$ that is used only to
avoid considering the two equal quantities $|H(a) - H(b)|$ and
$|H(b) - H(a)|$ in the above sum.

To simplify the references to the $q$ coefficients later we introduce
an \emph{index set} $I$.  This set is defined to be $\{0\} \cup
\{(a, b) \mid a \prec b\}$.  Each index $i$ corresponds to a \emph{base
function} $f_i$ in the potential function: $0$ corresponds to the constant
function $\_ \mapsto 1$, and $(a,b)$ corresponds to $H \mapsto
|H(a) - H(b)|$.  Using these notations we can rewrite the above
equality as
$$
\Phi(H) = \sum_{i \in I} q_i f_i(H).
$$
From now on, we will write $ab$ to denote the index $(a,b)$.

\subsection{Proof Rules}

\begin{figure}[ht!]
\begin{mathpar}

\Rule{Q:Skip}
{}
{ \Gamma; Q + M_k \vdash \code{skip} \dashv Q; \Gamma }

\and \Rule{Q:Assert}
{}
{ \Gamma; Q + M_a \vdash \code{assert}(C) \dashv Q; \Gamma, C }

\and \Rule{Q:While}
{ \Gamma, C ; Q - M_w^1 \vdash S \dashv M_w^2 + Q; \Gamma }
{ \Gamma; Q \vdash \code{while}(C)~S \dashv Q - M_w^3; \Gamma, \neg C }

\and \Rule{Q:Set}
{ \forall u,~q_{yu} = q'_{xu} + q'_{yu}
\\ q'_{xy} \in \mathbb Q_0^+
}
{ \Gamma[x/y]; Q + M_s \vdash x \gets y \dashv Q'; \Gamma }

\and \Rule{Q:Incr}
{ \mathcal U = \left\{ u \mid \Gamma \models x \pm y \in \inter x u \right\}
\\ \textstyle q'_y = q_y
      + \sum_{u \in \mathcal U} q_{xu}
      - \sum_{v \not\in \mathcal U} q_{xv}
}
{ \textstyle
  \Gamma[x/x \pm y]; Q + M_s \vdash x \gets x \pm y \dashv Q'; \Gamma
}

\and \Rule{Q:If}
{ \Gamma, C; Q - M_c^1 \vdash S_1 \dashv Q'; \Gamma'
\\ \Gamma, \neg C; Q - M_c^2 \vdash S_2 \dashv Q'; \Gamma'
}
{ \Gamma; Q \vdash \code{if} (C)~S_1~\code{else}~S_2 \dashv Q'; \Gamma' }

\and\Rule{Q:Seq}
{  \Gamma; Q - M_s^1 \vdash S_1 \dashv Q'; \Gamma'
\\ \Gamma'; Q' - M_s^2 \vdash S_2 \dashv Q''; \Gamma''
}
{ \Gamma; Q \vdash S_1;S_2 \dashv Q''; \Gamma'' }

\and \Rule{Q:Weak}
{ \Gamma_1 \models \Gamma_2
\\ Q_1 \succeq_\Gamma Q_2
\\ \Gamma_2; Q_2 \vdash S \dashv Q'_2; \Gamma'_2
\\ Q'_2 \succeq_\Gamma Q'_1
\\ \Gamma'_2 \models \Gamma'_1
}
{ \Gamma_1; Q_1 \vdash S \dashv Q'_1; \Gamma'_1 }

\and \Rule{Relax}
{ \mathcal L = \{ xy \mid \exists l_{xy},~\Gamma \models l_{xy} \le |x - y| \}
\\ \mathcal U = \{ xy \mid \exists u_{xy},~\Gamma \models |x - y| \le u_{xy} \}
%\\ q'_0 = q_0 + \sum_{i \in \mathcal U} u_i p_i - \sum_{i \in \mathcal L} l_i p_i
\\ \forall i \in \mathcal U,~ q'_i \ge q_i - p_i
\\ \forall i \in \mathcal L,~ q'_i \ge q_i + p_i
\\ \forall i \not\in \mathcal U \cup \mathcal L,~ q'_i \ge q_i
}
{ Q' + \sum_{i \in \mathcal U} u_i p_i - \sum_{i \in \mathcal L} l_i p_i  \succeq_\Gamma Q }

\end{mathpar}
\caption{Abstract proof rules of the quantitative analysis}
\label{fig:proof}
\end{figure}

The deductive system defined in~\pref{fig:proof} derives
judgements of the form $\Gamma; Q \vdash S \dashv Q'; \Gamma'$.
Its semantics are formally described in the soundness section but we
will give an intuition of its meaning here.

\paragraph{Quantitative part.}
The judgement is really two parts, one logical part, and one quantitative
part.  The quantitative part is, as shown in the overview section, a
representation of two potential functions $\Phi$ and $\Phi'$ using
two maps from indices to non-negative rational numbers
$Q, Q' : I \rightarrow \mathbb Q_0^+$, these maps are called
\emph{quantitative contexts}.
We note $q_i$ for $Q(i)$.  Since $Q$ specifies a value for each
index in $I$ it is clear that it defines a unique linear potential function
as described in the previous section. We define $H \mapsto \Phi_H(Q)$
to be the linear potential function canonically associated to $Q$.

The rules of~\pref{fig:proof} are designed to enforce a semantic
soundness similar to the one exposed in the overview section.  To
do this, the rules impose contraints on $Q$ and $Q'$.  To make the
presentation lighter, we use the following  set of conventions.
First, the context $Q'$ noted $Q \pm n$ is defined by $q'_0 = q_0 \pm n$
and $\forall i \neq 0,~q'_i = q_i$.  Second, for any index $i$ such that
the relation between $q_i$ and $q'_i$ is not specified as a side condition
of a rule, we assume that $q_i = q'_i$.  Third, we implicitely assume
all the side conditions enforcing that the coefficients $(q_i)_{i \in I}$ are
non-negative numbers. Finally, when we write $q_x$ for $q_{x0}$ where
0 is a program variable symbolizing the constant 0.

\paragraph{Logical part.}
The second part of judgements maintains an abstract program state.
This should be seen as a very weak Hoare logic.  It has to be weak because
we want to automate it trivially and it can be weak because we only use
it to get very local and simple knowledge.  The specifics of logical assertions
in $\Gamma$ are left to the implementation section.
%
The purpose of this logical state is to help the rules
make some sound decisions when relating coefficients of quantitative
contexts.

Syntactically $\Gamma$ is a list of program conditions.  Because of the
rules {\sc Q:While} and {\sc Q:If}, we require the program conditions
to be closed by negation.  Semantically we define the validity
$H \models \Gamma$ of $\Gamma = C_1, \dots, C_n$ on a given
heap $H$ to be $\bigwedge_i \llbracket C_i \rrbracket_H = \top$.
%
The logical state
is maintained using regular Hoare style rules.
The semantic entailment on logical states is noted $\Gamma \models \Delta$,
its meaning is that $\forall H,~ H\models\Gamma \implies H\models\Delta$.

%\paragraph{Explanation of selected rules.}

\section{Soundness Theorems}

The soundness of a judgement relates the cost semantics with the potential
functions derived using our proof system.  Depending on the outcome of
the semantics, we give different soundness statements.
But, before tackling the issue of soundness let us remind and define some
convenient notations.
\begin{itemize}
\item $H \mapsto \Phi_H(Q)$ for $Q$ a quantitative context, is the
unique linear potential function that has $Q(i)$ as a coefficient for the
base function $f_i$.
\item $H \models \Gamma$ is true if and only if all the assertions in
$\Gamma$ are true on the heap $H$.
\end{itemize}

Ideally we would
like our judgements to be meaningful even in the case of a program
crash.  More precisely, we want the first potential function to be enough
to pay for the execution of the program up to the crash point.  This is
formally stated in the following theorem.

\begin{theorem}[Busy case]
For any $S$, $H$, $q$, $q'$, $\Gamma$, $\Gamma'$, $Q$ and $Q'$
such that $H \models \Gamma$, $\Gamma; Q \vdash S \dashv Q'; \Gamma'$,
  and $(H, S) \Downarrow_{(q,q')} \circ$,
we have $q \le \Phi_H(Q)$.
\end{theorem}
\begin{proof}
Simple application of the terminating case proved afterwards.
\end{proof}

In order to prove the harder case for terminating programs, need to first
focus on the only structural rule of the proof system: {\sc Q:Weak}.  This
rule can be applied anywhere in the derivation, it is not syntax directed.
To prove the soundness of this rule, we need a lemma that gives a
semantic explanation of the $\succeq_\Gamma$ relation.

\begin{lemma}[Relax]
If $Q' \succeq_\Gamma Q$, for any heap $H$ such that $H \models \Gamma$,
we have $\Phi_H(Q') \ge \Phi_H(Q)$.
\end{lemma}
\begin{proof}
We observe the following inequations.
\begin{align*}
\Phi_H(Q') &= \sum_i q'_i f_i(H) \\
&\ge q_0 + \sum_{i\in\mathcal U} u_i p_i - \sum_{i\in\mathcal L} l_i p_i
  + \sum_{i\in\mathcal U} (q_i - p_i) f_i(H) + \sum_{i\in\mathcal L} (q_i + p_i) f_i(H)
  + \sum_{i\not\in \mathcal U \cup \mathcal L} q_i \\
 &\ge \Phi_H(Q)
  + \sum_{i\in\mathcal U} (u_i - f_i(H)) p_i
  + \sum_{i\in\mathcal L} (f_i(H) - l_i) p_i \\
 &\ge \Phi_H(Q).
\end{align*}
Since $H \models \Gamma$, using the transitivity of $\models$ and
the definition of $\mathcal U$ and $\mathcal L$ we get
$\forall i\in\mathcal L,~ l_i \le f_i(H)$ and
$\forall i\in\mathcal U,~ f_i(H)\le u_i$.
These two inequalities justify the last step.
\end{proof}

Using this lemma, we can tackle the hardest theorem of soundness.  This theorem
needs a strong statement to make the proof by induction go through and defines
the complete meaning of a judgement $\Gamma; Q \vdash S \dashv Q'; \Gamma'$.
In a similar fashion than the busy case, the initial quantitative context is enough
to perform the whole computation: $q \le \Phi_H(Q)$.  But additionally, the
two quantitative contexts are also related to the resource consumption $q - q'$
of the piece of program executed.  This second requirement exactly matches
the semantic condition described in the overview section: it says that the
initial potential $\Phi_H(Q)$ is enough to both pay for the resource
consumption of the program $q - q'$ and pay for the potential of the resulting
heap $\Phi_{H'}(Q')$.

The proof of this result if very technical but basically goes through by induction
on the big-step evaluation.  Only a representative subset of all the cases
is present in the following proof.


\begin{theorem}[Terminating case]
For any $S$, $H$, $H'$, $q$, $q'$, $\Gamma$, $\Gamma'$, $Q$ and $Q'$ such
that $\Gamma; Q \vdash S \dashv Q'; \Gamma'$ and $H \models \Gamma$ and
$(H,s) \Downarrow_{(q,q')} H'$, we have
$H' \models \Gamma'$ and
$q \le \Phi_H(Q)$ and
$\Phi_H(Q) - \Phi_{H'}(Q') \ge q - q'$.
\end{theorem}

\begin{proof} By nested induction on the big step semantics and on the
judgement derivation (to account for the structural rule {\sc Q:Weak}).

\begin{itemize}

\item Case {\sc S:Skip}.
  We have $(H, \code{skip}) \Downarrow_{M_k} H$.
  By assumption, $H \models \Gamma$. \\
  Since $\Phi_H(Q + M_k) = \Phi_H(Q) + M_k$,
  \begin{itemize}[topsep=0pt]
  \item
    if $M_k \ge 0$ we have,
      $M_k \le \Phi_H(Q + M_k)$ and
      $\Phi_H(Q + M_k) - \Phi_H(Q) \ge M_k - 0$;
  \item
    if $M_k < 0$ we have,
      $0 \le \Phi_H(Q + M_k)$ and
      $\Phi_H(Q + M_k) - \Phi_H(Q) \ge 0 - (-M_k)$.
  \end{itemize}

\item Case {\sc S:Seq}.
  By transitivity and induction hypothesis, it is
  easy to get $H'' \models \Gamma''$.  Now we prove
  the quantitative part of the theorem.  We have
  $(q, q') = M_s^1 \cdot (q_1, q'_1) \cdot M_s^2 \cdot (q_2, q'_2)$.
  We assume $\forall i \in \{1, 2\},~M_s^i \ge 0$, the other cases
  are similar.
  \begin{itemize}[topsep=0pt]
  \item
    The first case is when $q'_1 > M_s^2 + q_2$ then $q = M_s^1 + q_1$
    and since $q_1 \le \Phi_H(Q - M_s^1)$ (by induction) we have
    $q \le \Phi_H(Q)$.
  \item
    The second case is when $q'_1 \le M_s^2 + q_2$ then $q = M_s^1 + q_1 + M_s^2 + q_2 - q'_1$. \\
    Since $\Phi_H(Q - M_s^1) - \Phi_{H'}(Q') \ge q_1 - q'_1$
    and $q_2 \le \Phi_{H'}(Q' - M_s^2)$ we have:
    \begin{align*}
    q &\le M_s^1 + \Phi_H(Q - M_s^1) - \Phi_{H'}(Q') + M_s^2 + q_2 \\
      &= \Phi_H(Q) + (q_2 - \Phi_{H'}(Q' - M_s^2)) \le \Phi_H(Q).
   \end{align*}
  \end{itemize}
  We obtain $\Phi_H(Q) - \Phi_{H''}(Q'') \ge q - q'$ by simply chaining
  the two induction hypothesis and using the fact that $q - q' =
  (q_1 + M_s^1 - q'_1) + (q_2 + M_s^2 - q'_2)$.

\item Case {\sc S:WhileStop}.
  This proof goes just like in the {\sc S:Skip} case.

\item Case {\sc S:WhileStep}.
  Here again we assume $\forall i \in \{1, 2, 3\},~M_w^i \ge 0$.
  \begin{itemize}[topsep=0pt]
  \item
    If $q'_1 > M_w^2 + q_2$ then $q = M_w^1 + q_1$ and by induction
    we get $q_1 \le \Phi_H(Q - M_w^1)$, so $q \le \Phi_H(Q)$.
  \item
    If $q'_1 \le M_w^2 + q_2$ then $q = M_w^1 + q_1 + M_w^2 + q_2 - q'_1$.
    And we proceed like in the sequence case.
  \end{itemize}
  We obtain $\Phi_H(Q) - \Phi_{H''}(Q - M_w^3) \ge q - q'$ using the identity
  $q - q' = (M_w^1 + M_w^2 + q_1 - q'_1) + q_2 - q'_2$ and the two
  induction hypothesis
  \begin{itemize}[topsep=0pt]
  \item $\Phi_H(Q - M_w^1) - \Phi_{H'}(Q + M_w^2) \ge q_1 - q'_1$ and
  \item $\Phi_{H'}(Q) - \Phi_{H''}(Q - M_w^3) \ge q_2 - q'_2.$
  \end{itemize}

\end{itemize}
\end{proof}

\section{Implementation}

\subsection{Logical Annotations Generation}

\paragraph{Program conditions.}
The first pass in our OCaml implementation generates the
logical annotations $\Gamma$ around all program statements.
These annotations are used later on by the quantitative
analysis to generate contraints.
%
The first design choice we had to make is to give a fixed form
to program conditions.  Because of their generality and because
they are closed under negation, we decided to consider
inequalities over linear combinations as program conditions.
\begin{mathpar}
C := L < L \mid L > L \mid L \le L \mid L \ge L
\and
L := x \mid k \mid k * L \mid L + L \mid L - L
\end{mathpar}
A Condition is represented internally as a pair of one constant
$k \in \mathbb N$ and one map $m : V \rightarrow \mathbb N$.
Its intended semantics is
$$
\llbracket m \rrbracket(H) = k + \sum_v m_v \cdot H(v) \le 0.
$$
Note that conditions of this shape are closed by negation, as showed
by the following equivalence
$$
\neg (k + \sum_v m_v \cdot H(v) \le 0)
\Leftrightarrow (1-k) + \sum_v -m_v \cdot H(v) \le 0
$$
We can also note that all the ``questions'' asked to the logic
assertions during a proof derivation can be described using
linear combinations.  Conveniently enough, this language of
assertions is exactly Presburger's arithmetic.  This means that
we have a sound and complete decision procedure to answer
all the questions asked during the analysis.

\paragraph{Logical contexts.}
The second problem related to the logic is how to figure out
the logical contexts $\Gamma$ for each program statement.
This is a question that is answered by abstract interpretation
but, to show that our work really does not need the full power
of abstract interpretation, we designed and implemented a very
simple top-down approach: starting from a statement $S$ and
a pre-condition $\Gamma$ (given as a list of inequalities), we
generate $\mathcal P(S, \Gamma)$ a postcondition for $S$ using the
following equations:

\begin{align*}
\mathcal P(\code{skip}, \Gamma) &:= \Gamma \\
\mathcal P(x \gets y, \Gamma) &:= \{ e \le 0 \mid (e \le 0) \in \Gamma \land \{x\} \# e \} \\
\mathcal P(x \gets x + y, \Gamma) &:= \Gamma[x / x - y] \\
\mathcal P(x \gets x - y, \Gamma) &:= \Gamma[x / x + y] \\
\mathcal P(\code{assert}(C), \Gamma) &:= \Gamma,C \\
\mathcal P(S_1; S_2, \Gamma) &:= \mathcal P(S_2, \mathcal P(S_1, \Gamma)) \\
\mathcal P(\code{if} (C)~S_1~\code{else}~S_2) &:=
  \mathcal P(S_1, \Gamma,C) \lor \mathcal P(S_2, \Gamma,\neg C) \\
\mathcal P(\code{while}(C)~S, \Gamma) &:=
  \Gamma \lor (\neg C, \mathcal P(S, (C, \{ e \le 0 \mid (e \le 0) \in \Gamma \land {\rm set}(S) \# e \})))
\end{align*}
We used the notation ${\rm set}(S)$ to designate the set of variables
assigned to in the statement $S$ and $s\#e$ to mean that variables
in $s$ do not appear in the linear combination $e$.  Because contexts
are simply conjunction of conditions we must define $\Gamma \lor \Delta$
as a function on contexts:
$$
\Gamma \lor \Delta := \{ e \le 0 \mid (e \le 0) \in \Gamma \cup \Delta \land \Gamma \models (e \le 0) \land \Delta \models (e \le 0) \}.
$$
It is important to note that the $\mathcal P$ function defined above
is not performing any fixpoint computation.  Despite this flagrant
lack of sophistication our complete analysis is still able to figure out some
tricky invariants as exemplified earlier.
Finally, to generate annotations for the whole program $S$,
we run $\mathcal P(S,\bullet)$ (where $\bullet$ is the empty context)
and memorize all the intermediate results.  In practice, these results are stored in
a table that can be referenced at any time during the analysis to get the
pre and post-condition of any statement in the program.

\paragraph{Decision procedure.}
To decide logical entailment $\Gamma \models \Delta$ we use
Presburger's decision procedure.  But what this procedure gives us
is in fact only an answer the the question: Is there any heap $H$
such that $H \models \Gamma$.  While this sentence is existential
the question $\Gamma \models \Delta$ is universal.  To use the
decision procedure, we use the well known trick of double negation.
\begin{align*}
\Gamma \models C
&\Leftrightarrow \neg\neg (\Gamma \models C) \\
&\Leftrightarrow \neg\neg (\forall H, H\models\Gamma \implies H\models C) \\
&\Leftrightarrow \neg \exists H, H\models\Gamma \land \neg H\models C \\
&\Leftrightarrow \neg \exists H, H\models\Gamma \land H\models \neg C \\
&\Leftrightarrow \neg \exists H, H\models (\Gamma, \neg C)
\end{align*}
Thus, to know if $\Gamma \models C$ we ask the decision
procedure if $\Gamma, \neg C$ is satisfiable and negate the result.
When we want to know about $\Gamma \models \Delta$ we simply ask
one question for each of the conjuncts of $\Delta$.

\subsection{Linear Contraints Generation}

\paragraph{Generating a linear program.}
Because figuring out the coefficents that are in $Q$ right away is a hard
problem, we separate the search of a derivation in two steps.  As a first
step we go through the whole program and apply inductively the rules
given in the proof system section.  During this process our tool uses
symbolic names for the coefficients $(q_i)$.  Every time a side condition
must be satisfied by these coefficients, it is recorded using the symbolic
names in a global list.  When
the list of all constraints that must be satisfied by the coefficients is
collected, we feed it to an off the shelf LP-solver.  If the solver successfully
finds a solution to the given problem, we know that a derivation
exists for the considered program.  We can even ask the solver
for the initial $Q$ and get a resource bound for the program.
If the LP-solver fails to find a solution, an error is reported to the
user.

\paragraph{Making the proof system algorithmic.}
One problem with the proof system as described is that it is not
fully syntax directed.  The rule {\sc Q:Weak} can be applied at
all times.  However, if we apply this weakening rule at every step
of the derivation, the number of generated constraints and
the number of queries to the logic system explode.  This blowup
causes efficiency problems.  To avoid these performance problems
our implementation only applies the weakening rules at some
strategic points in the derivation.  More specifically, at sequence
points, at the beginning of while loops and around conditional
statements.  So far, this selection of weakening spots has been
driven by practice but it might be possible to prove that these
program points are the only ones where weakening is needed.


\section{Polynomial Potential}

\paragraph{Index Sets}

Let $V$ be a set of variables.  An \emph{index} $I \in \ind(V)$ is a
family that maps two-element sets of variables to natural numbers,
that is,
$$
I = (i_{\{x,y\}})_{\{x,y\} \subseteq V} \; .
$$
%
We identify a family $I$ with the set $\{ (\{x,y\},i_{\{x,y\}})
\mid \{x,y\} \subseteq V\}$.

Let $\ind(V)$ denote the set of all such indices.  We write $\ind$
instead of $\ind(V)$ if the set of variables $V$ is fixed or obvious
from the context.
%
We assume that allways $0 \in V$ and sometimes write $i_x$ instead of $i_{\{x,0\}}$.

The \emph{degree} $\deg(I)$ of an index $I = (i_{\{x,y\}})_{\{x,y\}
  \subseteq V}$ is defined as
$$
\deg(I) = \sum_{\{x,y\} \in V} i_{\{x,y\}} \;.
$$
We define $\ind_k(V) = \{ I \mid I \in \ind(V) \text{ and } \deg(i) \leq k
\}$ to be the set of indices of degree at most $k$.

\paragraph{Resource Polynomials}

Let $V$ be a set of variables.  An index $I \in \ind(V)$ denotes a
\emph{base polynomial} $P_I : \states \to \N$ for $V$ that maps a
program state $H$ to product of binomial coefficients (a natural
number).  We define
$$
P_I(H) = \prod_{{\{x,y\}} \subseteq V} \binom {|H(x){-}H(y)|} {i_{\{x,y\}}} \; .
$$
%
A \emph{resource polynomial} $R$ for the variable set $V$ is a
non-negative linear combination of the base polynomials for $V$.

\paragraph{Potential Annotations}

A \emph{potential annotation} for the variable set $V$ is a family
$$Q = (q_I)_{I \in \ind(V)}$$ 
of non-negative rational numbers.  Such an annotation denotes the
resource polynomial $R_Q$ that is defined by
$$
R_Q(H) = \sum_{I \in \ind(V)} q_I \cdot P_I(H) \; .
$$
%
We say that the potential annotation $Q$ is of degree $k$ if $q_I = 0$
for $I \in \ind(V)$ with $\deg(I) > k$.

\paragraph{Additive Shifts}

Let $Q$ be a potential annotation for a variable set $V$ and let
$\{x,y\} \subseteq V$ be a two-element variable set.  The
\emph{additive shift} with respect to $\{x,y\}$ is a potential
annotation $\shift_{\{x,y\}}(Q) = (q'_I)_{I \in \ind(V)} $ for $V$
that is defined through
$$
q'_I = q_I + q_{I^{\{x,y\}{+}1}} \; .
$$
For an index $I = (i_{\{x,y\}})_{\{x,y\} \subseteq V}$ we use the
notation $I^{\{x,y\}{+}k}$ to denote the index
$(i'_{\{x,y\}})_{\{x,y\} \subseteq V}$ such that
$$
i'_{\{t,u\}} = \left\{
  \begin{array}{ll}
    i_{\{t,u\}} + k  & \text{if } \{t,u\} = \{x,y\} \\
    i_{\{t,u\}} & \text{otherwise}
  \end{array}
\right.
\;.
$$
%
The additive shift for natural numbers reflects the identity 
\begin{equation}
\label{eq:shift}
\sum_{0 {\leq} i \leq {k}} q_i \binom{n+1}{i} = \sum_{0 {\leq} i \leq {k}} (q_i{+}q_{i+1}) \binom{n}{i}
\end{equation}
where $q_{k+1} = 0$.  It is used in the effect system if the
difference $n+1$ between two variables $x,y$ decreases by one.

\begin{lemma} Let $V$ be a set of variables with $x,y \in V$ and let
  $H$ be a program state. Let $|H'(t) {-} H'(u)| = |H(t) {-} H(u)|$
  for $\{t,u\} \neq \{x,y\}$ and let $|H'(x) {-} H'(y)| = |H(x) {-}
  H(y)| - 1$.
  %
  If $Q' = \shift_{\{x,y\}}(Q)$ then $R_Q(H) = R_{Q'}(H')$.
\end{lemma}

We now study the effect of multiple simultaneous shifts.  Let $Q$ be a
resource annotation for a variable set $V$ and let $U_1,\ldots,U_n
\subseteq V$ with $|U_i| = 2$ for all $i$ and $U_i \neq U_j$ for $i
\neq j$ be pairwise distinct two-element variable sets.  The
simulations additive shift $\shift_{U_1,\ldots,U_n}(Q)$ of $Q$ with
respect to $U_1,\ldots,U_n$ is defined by
$$
\shift_{U_1,\ldots,U_n}(Q) = \shift_{U_1}( \cdots \shift_{U_n}(Q) \cdots ) \; .
$$
%
\begin{proposition}
  Let $V$ be a set of variables and let $U_1,\ldots,U_n$ be pairwise
  distinct two-element variable sets.  Let $|H'(x) {-} H'(y)| = |H(x)
  {-} H(y)|$ for $\{x,y\} \not\in \{U_1,\ldots,U_n\}$ and let $|H'(x)
  {-} H'(y)| = |H(x) {-} H(y)| - 1$ for $\{x,y\} \in
  \{U_1,\ldots,U_n\}$.
  %
  If $Q' = \shift_{U_1,\ldots,U_n}(Q)$ then $R_Q(H) = R_{Q'}(H')$.
\end{proposition}
%
As shown by the following lemma, the order in which the shifts for the
individual $U_i$ are applied is insignificant.
%
\begin{lemma}
  Let $\sigma : \{1,\ldots,n\} \to \{1,\ldots,n\}$ be a
  permutation. Then $\shift_{U_1,\ldots,U_n}(Q) =
  \shift_{U_{\sigma(1)},\ldots,U_{\sigma(n)}}(Q)$.
\end{lemma}
%
For reasons of efficiency in the constraint generation, we give a more
direct formula for the simultaneous shift.  Let $I \in \ind(V)$ and
let $U_1,\ldots,U_n$ be pairwise distinct two-element variable sets.
We define the index $I^{U_1,\ldots,U_n + k}$ as the family $(i'_{\{x,y\}})_{\{x,y\} \subseteq V}$ such that
$$
i'_{\{t,u\}} = \left\{
  \begin{array}{ll}
    i_{\{t,u\}} + k  & \text{if } \{t,u\} \in \{U_1,\ldots,U_n\} \\
    i_{\{t,u\}} & \text{otherwise}
  \end{array}
\right.  \;.
$$

%
\begin{lemma}
  Let $V$ be a variable set and let $U_1,\ldots,U_n$ be pairwise
  distinct two-element variable sets.
  %
  Let $Q = (q_I)_{I \in \ind(V)}$ be a resource annotation for
  $V$ and let $ Q' = (q'_I)_{I \in \ind(V)}$ where
  $$
  q'_I = \sum_{\{j_1,\ldots,j_m\} \subseteq \{1,\ldots,n\} } q_{I^{U_{j_1},\ldots,U_{j_m}+1}} \; .
  $$
  Then $Q' = \shift_{U_1,\ldots,U_n}(Q)$.
\end{lemma}




%%
% Note: Later we need to shift in many directions at once like 
%   Q' = shift_{x,y} (shift_{x,u} (Q))
% To do: Give a combined formula for that (concise constraint system).
%%


\section{NOTES}

\subsection{Things to say in the intro}

\paragraph{Quote of Knuth}

\begin{quotation}
  There's even a fundamental gap in the foundations of my main
  mathematical specialty, the analysis of algorithms. Consider, for
  example, a computer program that sorts a list of numbers into
  order. Thanks to the work of Floyd, Hoare, and others, we have
  formal definitions of semantics, and tools by which we can verify
  that sorting is indeed always achieved. My job is to go beyond
  correctness, to an analysis of such things as the program's running
  time: I write down a recurrence, say, which is supposed to represent
  the average number of comparisons made by that program on random
  input data. I'm 100\% sure that my recurrence correctly describes
  the program's performance, and all of my colleagues agree with me
  that the recurrence is "obviously" valid. Yet I have no formal tools
  by which I can prove that my recurrence is right. I don't really
  understand my reasoning processes at all! My student Lyle Ramshaw
  began to create suitable foundations in his thesis (1979), but the
  problem seems inherently difficult. Nevertheless, I don't lose any
  sleep over this situation.
\end{quotation}
http://www.informit.com/articles/article.aspx?p=2213858

\paragraph{to do}

\begin{itemize}
\item describe schedulability analysis for two schedulers
\item include program logic from the stack paper for arbitrary resources
\item what operational semantics to use? 
\item verify automatic tool with program logic
\item derive some crazy bounds (polynomial? logarithmic?) with the logic
\item logic can also be used by automation: put linear constraints in the precondition
\end{itemize}

\paragraph{Advanteges over previous work}

\begin{itemize}
\item integrates well with hand-derived bounds in a program logic
\item have formal soundness proof with respect to cost semantics
\item can attach certificate to the code (proof-carrying code)
\item can deal with resources that \emph{become available} (e.g, freeing memory) in an execution
\item derive global bounds for the program as a function of the inputs (as opposed to local loop bounds)
\item can handle advance control flow such as break, return, mutual recursive functions
\item do not rely on abstract-interpretation based invariant generation
\item show that constant factors are very precise with respect to the cost semantics
\item demonstrate that the analysis is applicable in the formal verification of system code
\end{itemize}


% \acks

% This research is based on work supported in part by NSF grants 1319671
% and 1065451, and DARPA grants FA8750-10-2-0254 and FA8750-12-2-0293.
% Any opinions, findings, and conclusions contained in this document are
% those of the authors and do not reflect the views of these agencies.

\cleardoublepage
\appendix

\section{Catalog of Automatically Analyzed Programs}
\label{app:cat}

In this appendix we provide a non-exhaustive catalog of classes of
programs that can be automatically analyzed by our system.  For
simplicity, we use a cost metric that counts cost $1$ for assignments
and cost $0$ for all other operations.  Sometimes we also use the
\emph{ticks metric} if we want to discuss features like returning
resources.  Of course, the examples can also be analyzed with any other
cost metric.

We assume that free variables in the code snippets are the inputs to
the program. Some of the examples contain constants on which the
computed bound depends.  These constants are randomly chosen to
present an example but the analysis works for other constants as well.
Note however that it is sometimes crucial that constants are positive
(or negative) and that the same constant at different places.

\newlength{\progwidth}

\begin{figure*}[t!]
\setlength{\progwidth}{.24\linewidth}
  \centering

  \begin{minipage}[b]{\progwidth}
    \begin{center}
   \begin{lstlisting}
  while (z-y>0) {
    y = y+1;
  }
  while (y>9) {
    y=y-10;
  }
   \end{lstlisting}

$1.1|[y,z]| + 0.1|[0,y]|$
\\[.7\baselineskip] 
      {\bf t08}
    \end{center}
  \end{minipage}
%
%
%
  \begin{minipage}[b]{\progwidth}
    \begin{center}
   \begin{lstlisting}
  while (x-y>0) {
    if (*)
      y=y+1;
    else
      x=x-1;
  }
   \end{lstlisting}

$|[y,x]|$
\\[.7\baselineskip]
      {\bf t10}
    \end{center}
  \end{minipage}
%
%
%
  \begin{minipage}[b]{\progwidth}
    \begin{center}
   \begin{lstlisting}
  while (x>0) {
    x=x-1;
    if (*) 
      y=y+1;
    else {
      while (y>0)
	y=y-1;
    }
  }
   \end{lstlisting}

$3|[0,x]| + |[0,y]|$
\\[.7\baselineskip]
      {\bf t13}
    \end{center}
  \end{minipage}
%
%
%
  \begin{minipage}[b]{\progwidth}
    \begin{center}
   \begin{lstlisting}
while (n<0) {
  n=n+1;
  y=y+1000;
  while (y>=100 && *){
    y=y-100;
  }
}
   \end{lstlisting}

$12|[n,0]| + 0.01|[0,y]|$
\\[.7\baselineskip]
      {\bf t27}
    \end{center}
  \end{minipage}
   \caption{Amortization and Compositionality (a)}
  \label{fig:cat1a}
\end{figure*}


\begin{figure*}[t!]
 \setlength{\progwidth}{.24\linewidth}
  \centering

  \begin{minipage}[b]{\progwidth}
    \begin{center}
   \begin{lstlisting}
  while (x>0) {
    x=x-1;
    y=y+2;
  }
  while (y>0) {
    y=y-1;
  }
  while (y>0) {
    y=y+1;
  }
   \end{lstlisting}

$4|[0,x]| + |[0,y]|$
\\[.7\baselineskip]
      {\bf t07}
    \end{center}
  \end{minipage}%
%
%
%
  \begin{minipage}[b]{\progwidth}
    \begin{center}
   \begin{lstlisting}
  while (x>y) {
    x=x-1;
    x=x+1000;
    y=y+1000;
  }
  while (y>0) {
    y=y-1;
  }
  while (x<0) {
    x=x+1;
  }
   \end{lstlisting}

$1004|[y,x]|+|[x,0]|+|[0,y]|$
\\[.7\baselineskip]
      {\bf t28}
    \end{center}
  \end{minipage}%
%
%
  \begin{minipage}[b]{\progwidth}
    \begin{center}
   \begin{lstlisting}
  assert (y>=0);
  while (x-y>0) {
    x=x-1;
    x=x-y;
    z=y;
    while (z>0) {
      z=z-1;
    }
  }
   \end{lstlisting}

$3|[0,x]|$
\\[.7\baselineskip]
      {\bf t15}
    \end{center}
  \end{minipage}
%
%
  \begin{minipage}[b]{\progwidth}
    \begin{center}
   \begin{lstlisting}
  assert (y>=0);
  while (x-y>0) {
    x=x-1;
    x=x-y;
    z=y;
    z=z+y;
    z=z+100;
    while (z>0) {
      z=z-1;
    }
  }
   \end{lstlisting}

$1+2|[0,x]|+103|[y,x]|$
\\[.7\baselineskip]
      {\bf t16}
    \end{center}
  \end{minipage}


   \caption{Amortization and Compositionality (b)}
  \label{fig:cat1b}
\end{figure*}


\begin{figure*}[t!]
 \setlength{\progwidth}{.24\linewidth}
  \centering

  \begin{minipage}[b]{\progwidth}
    \begin{center}
   \begin{lstlisting}
  while (i>100) {
    i--;
  }
  i=i+k+50;
  while (i>=0) {
    i--;
  }
   \end{lstlisting}

$52 + |[-1,i]| + |[0,k]|$
\\[.7\baselineskip]
      {\bf t19}
    \end{center}
  \end{minipage}%
%
%
%
  \begin{minipage}[b]{\progwidth}
    \begin{center}
   \begin{lstlisting}
  while (x<y) {
    x=x+1;
  }
  while (y<x) {
    y=y+1;
  }
   \end{lstlisting}

$|[x,y]|+|[y,x]|$
\\[.7\baselineskip]
      {\bf t20}
    \end{center}
  \end{minipage}%
%
%
  \begin{minipage}[b]{\progwidth}
    \begin{center}
   \begin{lstlisting}
  while (x>0) {
    x=x-1;
    t=x;
    x=y;
    y=t;
  }
   \end{lstlisting}

$4|[0,x]|+4|[0,y]|$
\\[.7\baselineskip]
      {\bf t30}
    \end{center}
  \end{minipage}
%
%
  \begin{minipage}[b]{\progwidth}
    \begin{center}
   \begin{lstlisting}
  flag=1;
  while (flag>0) {
    if (n>0 && *) {
      n=n-1;
      flag=1;
    } else
      flag=0;
  }
   \end{lstlisting}

$2 + 2|[0, n]|$
\\[.7\baselineskip]
      {\bf t47}
    \end{center}
  \end{minipage}

   \caption{Amortization and Compositionality (c)}
  \label{fig:cat1c}
\end{figure*}

\paragraph{Amortization and Compositionality}

Figures~\ref{fig:cat1a}, \ref{fig:cat1b}, and~\ref{fig:cat1c} show
code snippets that need amortization are compositionality to obtain a
whole program bound.

Example \emph{t07} demonstrates two different features of the
analysis.  For one thing it shows that we can precisely track size
changes inside loops.  In the first loop, we increment $y$ by $2$ in
each of the $|[0,x]|$ iterations.  An in the second loop, we decrement
$y$.  For another thing it shows that we automatically recognize dead
code if we find conflicting assertions on a branching path: After the
second loop we know $y \leq 0$ and as a result can assign arbitrary
potential inside the third loop where we know that $y>0$.  As a
result, we obtain a tight bound.

Example \emph{t08} shows the ability of the analysis to handle
negative and non-negative numbers.  Note that there are no
restrictions on the signs of $z$ and $y$.  We also see again that we
accurately track the size change of $y$ in the first loop.
Furthermore, \emph{t08} shows that we do not handle the constants $1$
or $0$ in any special way.  In all examples you could replace $0$ and
$1$ with other constants like we did in the second loop and still
derive a tight bound.  The only information, that the analyzer needs
is $y \geq c$ before assigning $y = y - c$.

In example \emph{t10} we also do not restrict the inputs $x$ and $y$.
They can be negative, positive, or zero.  The star {\tt *} in the
conditional, stands for an arbitrary assertion.  In each branch of the
conditional we can obtain the constant potential $1$ since the interval
size $|[y,x]|$ is decreasing.

Example \emph{t13} shows how amortization can be used to handle tricky
nested loops.  The outer loop is iterated $|[0,x]|$ times.  In the
conditional, we either (the branching condition is again arbitrary)
increment the variable $y$ or we execute an inner loop in which $y$ is
counted back to $0$.  The analysis computes a tight linear bound for
this program.  Again, the constants $0$ and $1$ in the inner loop can
as well be replace by something more interesting, say $9$ and $10$
like in example \emph{t08}.  Then we still obtain a tight linear
bound.

Example \emph{t27} is similar to example \emph{t13}.  Instead of
decrementing the variable $x$ in the outer loop we this time increment
the variable $n$ till $n = 0$.  In each of the $|[n,0]|$ iterations,
we increment the variable $y$ by $1000$.  We then execute an inner
loop that increments $y$ by $100$ until $y=0$.  The analysis can
derive that only the first execution of the inner loop depends on the
initial value of $y$.  We again derive a tight bound.

Example \emph{t28} is particularly interesting.  In the first loop we
decrement the size $|[y,x]|$.  However, we also shift the interval
$[y,x]$ to the interval $[y+1000,x+1000]$.  The analysis can derive
that this does not change the size of the interval and computes the
tight loop bound $3|[y,x]|$.  The additional two loops are in the
program to show that the size tracking in the first loop works
accurately.  The second loop is executed $|[0,y]| + 1000|[y,x]|$ times
in the worst case.  The third loop is executed $|[x,0]| + |[y,x]|$ in
the worst case (if $x$ and $y$ are negative).

Sometimes we need some assumptions on the inputs in order to derive a
bound.  Example \emph{t15} is such a case.  We assume here that the
input variable $y$ is non-negative and write \code{assert(y>=0)}.  The
semantic of \code{assert} is that it has no effect if the assertion is
true and that the program is terminated without further cost
otherwise.  If we enter the loop then we know that $x>0$ and we can
obtain constant potential from the assignment \code{x=x-1}.  After the
assignment we know that $x\geq y$ and $y\geq 0$.  As a consequence, we
can share the potential $3|[0,x]|$ before the assignment \code{x=x-y}
between $3|[0,x]|$ and $3|0,y|$ after the assignment.  In this way, we
derive a tight linear bound.

Example \emph{t16} is an extension of example \emph{t15}. We again
assume that $y$ is non-negative and use the same mechanism to iterate
the outer loop as in \emph{t15}.  In the inner loop, we also count the
variable $z$ down to zero and perform $|[0,z]|$ interations.  However,
instead of assigning \code{z=y}, we assign \code{z=2y+100}.  The analysis
computes the tight linear bound $1+2|[0,x]|+103|[y,x]|$.  The
assignment of potential to the size interval $|[y,x]|$ in instead of
$|[0,x]|$ is a random choice of the LP solver.  Other optimal
solutions of the linear program that the analyzer gernerates for
\emph{t15} would result in bounds such as $1+105|[0,x]|$ or
$1+6|[0,x]|+99|[y,x]|$.  In fact, the $105$ potential units can be
shared arbitrarily between the interval sizes $|[y,x]|$ and $|[0,x]|$
as long as more than $1$ unit is assigned to $|[0,x]|$.  Since we know
that, $y\geq 0$ the choice of the LP solver is optimal but this seems
to be a coincidence.

Example \emph{t19} demonstrates another nice property that demonstates
the compositionaliy of the analysis.  The program consists of two
loops that decrement a variable $i$.  In the first loop, $i$ is
decremented down to 100 and in the second loop $i$ is increment
further down to $-1$.  However, between the loops we assign
$i=i+k+50$.  So in total the program performs $52 + |[-1,i]| +
|[0,k]|$ increments.  Our analysis finds this tight bound because our
amortized analysis naturally takes into account the relation between
the two loops.  Other techniques might derive a more conservative
bound such as $52 + |[-1,i]| + |[0,k]| + |[100,i]|$.

Example \emph{t20} shows how we can handle programs in which bounds
contain absolute values like $|x-y|$.  The first loop body is only
executed if $x<y$ and the second loop body is only executed if $y<x$.
The analyzer finds a tight bound.

At first sight, example \emph{t30} appears to be a simple loop that
decrements the variable $x$ down to zero.  However, a closer look
reveals that the loop actually decrements both input variables $x$ and
$y$ down to zero before terminating.  In the loop body, first $x$ is
decremented by one.  Then the values of the variables $x$ and $y$ are
switched using the local variable $t$ as a buffer.  Our analysis
infers the tight bound $4|[0,x]|+4|[0,y]|$.

Example \emph{t47} demonstrates how we can use integers as Booleans to
amortize the cost of loops that depend on boolean flags.  The outer
loop is executed as long as the variable flag is ``true'', that is,
\code{flag>0}.  Inside the loop, there is a conditional that either
(if $n>0$) decrements $n$ and assigns \code{flag=1}, or (if $n\geq0$)
leaves n unchanged and assigns \code{flag=0}.  The analyzer computes
the tight bound $2 + 2|[0, n]|$.  The potential in the loop invariant
is $|[0,\mathit{flag}]| + 2|[0, n]|$.  In the \emph{then} branch of
the conditional, we use the potential $2|[0, n]|$ and the fact that
$n>0$.  In the \emph{else} branch, we use the potential
$|[0,\mathit{flag}]|$ and the fact that $\mathit{flag}=1$.




\begin{figure*}[t!]
 \setlength{\progwidth}{.24\linewidth}
  \centering
  \begin{minipage}[b]{\progwidth}
    \begin{center}
   \begin{lstlisting}
  while (n>x) {
    if (m>y) 
      y = y+1;
    else
      x = x+1;
  }
   \end{lstlisting}

$|[x, n]| + |[y, m]|$
\\[.7\baselineskip]
      {\bf fig2\_1}
    \end{center}
  \end{minipage}
%
%
  \begin{minipage}[b]{\progwidth}
    \begin{center}
   \begin{lstlisting}
  while (x<n) {
    if (z>x)
      x=x+1;
    else
      z=z+1;
  }
   \end{lstlisting}

$|[x, n]| + |[z, n]|$
\\[.7\baselineskip]
      {\bf fig2\_2}
    \end{center}
  \end{minipage}
%
%
  \begin{minipage}[b]{\progwidth}
    \begin{center}
   \begin{lstlisting}
  while (x<n) {
    while (y<m) {
      if (*) break;
      y=y+1;
    }
    x=x+1;
  }
   \end{lstlisting}

$|[x, n]| + |[y, m]|$
\\[.7\baselineskip]
      {\bf nested\_multiple}
    \end{center}
  \end{minipage}
%
%
  \begin{minipage}[b]{\progwidth}
    \begin{center}
   \begin{lstlisting}
  x=0;
  while (x<n) {
    x=x+1;
    while (x<n) {
      if (*) break;
      x=x+1;
    }
  }
   \end{lstlisting}

$1 + |[0, n]|$
\\[.7\baselineskip]
      {\bf nested\_single}
    \end{center}
  \end{minipage}

   \caption{Examples from Gulwani et al's SPEED~\cite{GulwaniMC09}} (a)
  \label{fig:cat2a}
\end{figure*}

\begin{figure*}[t!]
 \setlength{\progwidth}{.24\linewidth}
  \centering
  \begin{minipage}[b]{\progwidth}
    \begin{center}
   \begin{lstlisting}
  x=0;
  while (x<n) {
    if (*) break;
    x=x+1;
  }
  while (x<n)
    x=x+1;
   \end{lstlisting}

$1 + |[0,n]|$
\\[.7\baselineskip]
      {\bf sequential\_single}
    \end{center}
  \end{minipage}
%
%
  \begin{minipage}[b]{\progwidth}
    \begin{center}
   \begin{lstlisting}
  x=0; y=0;
  while (x<n) {
    if (y<m)
      y=y+1;
    else
      x=x+1;
  }
   \end{lstlisting}
$2 + |[0, m]| + |[0, n]|$
\\[.7\baselineskip]
      {\bf simple\_multiple}
    \end{center}
  \end{minipage}
%
%
  \begin{minipage}[b]{\progwidth}
    \begin{center}
   \begin{lstlisting}
  x=0;
  while (x<n) {
    if (*)
      x=x+1;
    else 
      x=x+1;
   \end{lstlisting}

$1 + |[0,n]|$
\\[.7\baselineskip]
      {\bf simple\_single}
    \end{center}
  \end{minipage}
%
%
  \begin{minipage}[b]{\progwidth}
    \begin{center}
   \begin{lstlisting}
  x=0; y=0;
  while (*) {
    if (x<N) {
      x=x+1; y=y+1;
    } else if (y<M ) {
      x=x+1; y=y+1;
    } else
      break;
  }
   \end{lstlisting}

$2 + 2 |[0, M]| + 2 |[0, N]|$
\\[.7\baselineskip]
      {\bf simple\_single\_2}
    \end{center}
  \end{minipage}

   \caption{Examples from Gulwani et al's SPEED~\cite{GulwaniMC09} (b)}
  \label{fig:cat2b}
\end{figure*}





\begin{figure*}[t!]
 \setlength{\progwidth}{.24\linewidth}
  \centering
%
%
  \begin{minipage}[b]{\progwidth}
    \begin{center}
   \begin{lstlisting}
  assert n>0;
  assert m>0;
  va = n; vb = 0;
  while (va>0 && *) {
    if (vb<m) { 
      vb=vb+1; 
      va=va-1;
    } else {
      vb=vb-1;
      vb=0;
    }
  }
   \end{lstlisting}
$2 + 4|[0, n]|$
\\[.7\baselineskip]
      {\bf fig\_4\_2}
    \end{center}
  \end{minipage}
%
%
%
%
  \begin{minipage}[b]{\progwidth}
    \begin{center}
   \begin{lstlisting}
  assert (0<m);
  i = n;
  while (i>0 && *) {
    if (i<m)
      i=i-1;
    else
      i=i-m;
  }
   \end{lstlisting}
$|[0, n]|$
\\[.7\baselineskip]
      {\bf fig\_4\_4}
    \end{center}
  \end{minipage}
%
%
  \begin{minipage}[b]{\progwidth}
    \begin{center}
   \begin{lstlisting}
  assert (0 < m < n);
  i=m;
  while (0<i<n) {
    if (dir==fwd) i++;
    else i--;
  }
   \end{lstlisting}
$---$
\\[.7\baselineskip]
      {\bf fig\_4\_5}
    \end{center}
  \end{minipage}

   \caption{Examples from [GulwaniPLDI09]}
  \label{fig:cat2c}
\end{figure*}


\begin{figure*}[t!]
 \setlength{\progwidth}{.24\linewidth}
  \centering
%
%
  \begin{minipage}[b]{\progwidth}
    \begin{center}
   \begin{lstlisting}
  i=0;
  while (i<n) {
    j=i+1;
    while (j<n) {
      if (*) {
	_tick(1);
	j=j-1; n=n-1;
      }
      j=j+1;
    }
    i=i+1;
  }
   \end{lstlisting}
$|[0, n]| \text{ ticks}$
\\[.7\baselineskip]
      {\bf ex1}
    \end{center}
  \end{minipage}
%
%
  \begin{minipage}[b]{\progwidth}
    \begin{center}
   \begin{lstlisting}
  while (n>0 && m>0) {
    n--; m--;
    while (nondet()) {
      n--; m++;
    }
  }
   \end{lstlisting}
$---$
\\[.7\baselineskip]
      {\bf ex2}
    \end{center}
  \end{minipage}
%
%
  \begin{minipage}[b]{\progwidth}
    \begin{center}
   \begin{lstlisting}
  while (n>0) {
    t = x;
    n=n-1;
    while (n>0) {
      if (*) break;
      n=n-1;
    }
  }
   \end{lstlisting}
$2 |[0, n]|$
\\[.7\baselineskip]
      {\bf ex3}
    \end{center}
  \end{minipage}
%
%
  \begin{minipage}[b]{\progwidth}
    \begin{center}
   \begin{lstlisting}
  flag=1;
  while (flag>0) {
    flag=0;
    while (n>0) {
      n=n-1;       
      flag=1;
    }
  }
   \end{lstlisting}
$2 + 3|[0, n]|$
\\[.7\baselineskip]
      {\bf ex4}
    \end{center}
  \end{minipage}


   \caption{Examples from [GulwaniPLDI10]}
  \label{fig:cat3a}
\end{figure*}


\paragraph{From  the Literature}

Our analyzer can derive almost all linear bounds for programs that
have been described as challenges in the literature on bound
generation.  We found only three programs with a linear bound for
which our analyzer could not find a tight bound.  The first one
(\emph{fig\_4\_5}) from [GulwaniPLDI09] requires \emph{path-sensitive
  reasoning} to derive a bound.  The other two, \emph{ex2} and
\emph{ex5} (see [GulwaniPLDI10]), from [GulwaniPLDI10] were described
to have a linear bound but seem to be non-terminating.

Examples \emph{fig2\_1} and \emph{fig2\_2} are taken from Gulwani et
al~\cite{Gulwani-speed}.  They are both handled by the SPEED tool but
require inference of a \emph{disjunctive invariant}.  In the abstract
interpretation community, these invariants are known to be notoriously
difficult to handle.
%
In example \emph{fig2\_1} we have one loop that first increments
variable $y$ up to $m$ and then increments variable $x$ up to $n$.  We
derive the tight bound $|[x, n]| + |[y, m]|$.
%
Example \emph{fig2\_2} is more tricky and trying to understand how it
works may be challenging.  However, with the amortized analysis in
mind, using the potential transfer reasoning, it is almost trivial to
prove a bound.  While the SPEED tool has to find a fairly
involved invariant for the loop, our tool is simply reasoning locally
and works without any clever tricks. We obtain the tight bound $|[x,
n]| + |[z, n]|$.

Example \emph{nested\_multiple} is similar to example \emph{fig2\_1}.
Instead of incrementing variable $y$ in the outer loop, $y$ is here
potentially incremented multiple times in each iteration of the outer
loop.  The idea of example \emph{nested\_single} is similar.  However,
instead of incrementing variable $y$ in the inner loop, we increment
$x$, the counter variable of the outer loop. Our analyzer derives a
tight bound for both programs.  Note that a star \emph{*} in a
branching condition denotes an arbitrary boolean condition that might
of course change while iterating (non-deterministic choice).

Example \emph{sequential\_single} is like example
\emph{nested\_single}.  The only difference is that the inner loop of
\emph{nested\_single} is now evaluated after the outer loop.  Example
\emph{simple\_multiple} is a variant of example \emph{fig2\_1} and
\emph{simple\_single} is a simple variant of \emph{nested\_single}.
We derive tight bounds for all aforementioned programs.

Example \emph{simple\_single\_2} uses conditionals and a \code{break}
statement to control loop iterations.  If $x<N$ then variables $x$ and
$y$ are incremented.  Otherwise, if $y<M$ then the same increment is
executed.  If $y\geq M$ and $x\geq N$ then the loop is terminated with
a break.  Our tool computes the bound $2 + 2 |[0, M]| + 2 |[0, N]|$.
This bound is tight in the sense that there are inputs (such as $M =
-100$ and $N = 100$) for which the bound precisely describes the
execution cost.  However, SPEED can compute the more precise bound
$\max(N,M)$.  We currently cannot express this bound in our system.

Example \emph{fig\_4\_2} from [GulwaniPLDI09] is quite involved.
Amortized reasoning helps to understand how we derive the bound $4 +
4|[0, n]|$.  We start with potential $6 + 4|[0, n]|$ and use the
constant potential to pay for the two first assignments.  We use the
remaining potential $4 + 4|[0, n]|$ and the fact that $vb=0$ to
establish the potential $4 + 4|[0, n]| + 2|[0,vb]|$ that serves as a
loop invariant.  In the \emph{if branch} of the conditional, we use
the constant potential $4$ of the invariant to pay for the two
assignments ($2$ units) and the potential of $|[0,vb]|$ ($2$ units).
Since we also know that $|[0, n]|>0$ we obtain constant potential $4$
and establish the loop invariant again.  In the \emph{else} branch, we
use the potential $2|[0,vb]|$ and the fact $vb>0$ to obtain $2$
potential units to pay for the assignment.  Since we have $4$ constant
potential units left after the loop, we can subtract them from the
initial potential $6 + 4|[0, n]|$.

In example \emph{fig\_4\_4} it is essential that $m$ is positive.
That ensures that we can obtain constant potential for the interval
size $|[0,i]|$ in the \emph{else} branch of the conditional since
$|[0,i]|$ decreases.  Example \emph{fig\_4\_5} is one of the three
examples that we can not handle automatically.  The execution is
bounded because the boolean value of the test \code{dir==fwd} does not
change during the iteration of the loop.  As a result, the variable
$i$ is either counted down to $0$ or up to $n$.  Our tool cannot
handle example \emph{fig\_4\_5} because we don't do path sensitive
reasoning.  Note however that it would be more efficient to move the
test \code{dir==fwd} outside of the loop (this would be also done by
an optimizing compiler).  The resulting program could be analyzer by our
tool.

Example \emph{ex1} from [GulwaniPLDI10] specifically focusses on the
code in the \emph{if} statement.  So we use the \emph{tick metric} and
insert \code{_tick(1)} inside the if statement to derive a bound on
the number of times the code in the if statement is executed.  Note
that we cannot derive a bound for the whole program since the outer
loop is executed a quadratic number of times.  Nevertheless it is
straightforward to derive a bound on the number of ticks using the
amortized approach: In the if statement we know that $n>0$ and assign
\code{n=n-1}.  So we can use the potential of the interval size $|[0,n]|$
to pay for the tick.

We cannot analyze example \emph{ex2} for which previous work reported
a linear bound [GulwaniPLDI10].  The reason is that this example is
not terminating since the inner loop is not bounded.  The same is true
for example \emph{ex5} from the same paper, which is not listed here.
Finally, example \emph{ex2} is similar to example
\emph{nested\_single}, and \emph{ex3} is a variant of example
\emph{t47}.

\begin{figure*}[t!]
 \setlength{\progwidth}{.28\linewidth}
  \centering
%
%
  \begin{minipage}[b]{\progwidth}
    \begin{center}
   \begin{lstlisting}
void count_down (int x) {
  int a = x;
  if (a>0) {
    a = a-1;
    count_down(a);
  }
}
int copy (int x, int y) {
  if (x>0) {
    x = x-1;
    y = y+1;
    y=copy(x,y);
  };
  return y;
}
void _main (int x,int y) {
  y = copy (x,y);
  count_down(y);
}
   \end{lstlisting}

$1 + 4|[0, x]| + 2|[0, y]|$
\\[.7\baselineskip]
      {\bf t37}
    \end{center}
  \end{minipage}
%
%
  \begin{minipage}[b]{\progwidth}
    \begin{center}
   \begin{lstlisting}
void count_down (int x,int y)
{ int a = x;
  if (a>y) {
    a = a-1;
    count_up(a,y);
  }
}

void count_up (int x, int y)
{ int a = y;
  if (a+1<x) {
    a = a+2;
    count_down(x,a);
  }
}

void _main (int y, int z) {
  count_down(y,z);
}
   \end{lstlisting}

$1.67 + 1.33 |[z,y]|$
\\[.7\baselineskip]
      {\bf t39}
    \end{center}
  \end{minipage}
%
%
  \begin{minipage}[b]{\progwidth}
    \begin{center}
   \begin{lstlisting}
void produce () {
  while (x>0) {
    _tick(-1); x=x-1; y=y+1;
  }
}
void consume () {
  while (y>0) {
    y=y-1; x=x+1; _tick(1);
  }
}
void _main (int y, int z) {
  consume(); produce(); consume();
}
   \end{lstlisting}

$|[0, y]|$ ticks
\\[.7\baselineskip]
      {\bf t46}
    \end{center}
  \end{minipage}

   \caption{Programs with (recursive) functions}
  \label{fig:cat3}
\end{figure*}


\paragraph{Recursive Functions}

Our approach can naturally deal with mutually-recursive functions.
The recursion patterns can be exactly the same that are used in
iterations of loops.  In the following, we present three simple
examples that illustrate the analysis of functions.

Example \emph{t37} illustrates that the analyzer is able to perform
inter-procedural size tracking.  The function \code{copy} adds the
argument $x$ to the argument $y$ if $x$ is positive.  However, this
addition is done in steps of $1$ in each recursive call.  The function
\code{count\_down} recursively decrements its argument down to $0$.
The derived bound $1 + 4|[0, x]| + 2|[0, y]|$ is for the function
\code{\_main} in which we first add $x$ to $y$ using the function
\code{copy} and then count down the variable $y$ using the function
\code{count\_down}.  The derived bound is tight.

Example \emph{t39} uses mutual recursion.  The function
\code{count\_down} is similar to the function with the same name in
example \emph{t37}.  However, we do not count down to $0$ but to a
variable $y$ that is passed as an argument and we call the function
\code{count\_up} afterwards.  The function \code{count\_up} is dual to
\code{count\_down}.  Here, we count up $y$ by $2$ and recursively call
\code{count\_down}.  For the function $\_main$, which calls
\code{count\_down(y,z)}, the analyzer computes the tight bound $1.67 +
1.33 |[z,y]|$.

Example \emph{t46} shows a program that uses and returns resources.
Again, we use the \emph{tick metric} and the function \code{\_tick} to
describe the resource usage.  The function \code{produce} produces
$|[0,x]|$ resources, that is, in each of the $|[0,x]|$ iterations, it
receives one resource unit.  Similarly, the function \code{consume}
consumes $|[0,y]|$ resources.  The analyzer computes the tight bound
$|[0,y]|$ for the function \code{\_main}.  This is only possible since
amortized analysis naturally tracks the size changes to the variables
$x$ and $y$, and the interaction between \code{consume} and
\code{produce}.


\begin{figure*}[t!]
 \setlength{\progwidth}{.24\linewidth}
  \centering
%
%
  \begin{minipage}[b]{\progwidth}
    \begin{center}
   \begin{lstlisting}
 XXX
   \end{lstlisting}

$XXX$
\\[.7\baselineskip]
      {\bf Knuth-Morris-Pratt}
    \end{center}
  \end{minipage}
%
%
  \begin{minipage}[b]{\progwidth}
    \begin{center}
   \begin{lstlisting}
 XXX
   \end{lstlisting}

$XXX$
\\[.7\baselineskip]
      {\bf Greatest Common Divisor}
    \end{center}
  \end{minipage}
%
%
  \begin{minipage}[b]{\progwidth}
    \begin{center}
   \begin{lstlisting}
 XXX
   \end{lstlisting}

$XXX$
\\[.7\baselineskip]
      {\bf Quick Sort}
    \end{center}
  \end{minipage}

   \caption{Well-Known Algorithms}
  \label{fig:cat3}
\end{figure*}

\paragraph{Well-Known Algorithms}






\bibliographystyle{abbrvnat}
\bibliography{lit}




\end{document}

%%% Local Variables: 
%%% mode: latex
%%% mode: flyspell
%%% TeX-master: t
%%% End: 
