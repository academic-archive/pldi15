
To address in this second response:
+ non-linear operations deserve better explanation (0 potential trick) (#41D #41E)
+ link with other tools? (#41E #41F)

Already addressed but still mentioned:
+ linear bounds
+ short tests


+++++++++++++++++


We thank the reviewers for their positive and constructive feedback.
We respond to the new reviews point by point.

+++ Pont-By-Point Response

Review #41D:

> The fully automatic analysis can only infer linear bounds.

We addressed this comment in our first response in the point
'Non-linear bounds'.

> p. 6  “Non-linear operations such as x <- z * y or x <- a[y] are
> handled by assigning 0 to coefficients like qxa and qax that contain
> x after the assignment.”  Can you give an explanation for why this
> treatment is sound?

We currently don't track size changes in some operations. Yet, we
handle them in the analysis and ensure soundness by assigning zero
potential to the result.

For example, assignments such as x = a[y] or x = t*y are re-written by our
pre-processor as x = * where '*' represents an expression returning an
unknown integer.  Since the value of x is unknown after the assignment,
we do not know the size of intervals |[x, u]| and |[u, x]| for all u
(!= x).  Because these intervals are of unknown size we require that
their potential is 0, this way, we know that for any heap H, Phi(H) >=
Phi'(H[x <- *]), and therefore assignment of potential 0 is sound.

> Figure 4.  What are the M meta-variables? For example, M_l in (Q:Loop).

The M variables are parameters of the cost-semantics that we call
"resource metric".  They allow the semantics to work for different
resources.  It is defined at the end of page 8. We will move it
further to the front.

Review #41E:

> It seems the tool can only derive linear bounds, while existing tools
> can derive polynomial bounds. I suppose this is not a huge issue as
> the tighter linear bounds can be integrated within these other tools to
> computer tighter polynomial bounds.

We addressed this comment in our first response in the point
'Non-linear bounds'.

> Although this is a step in the right direction, resource bound inference
> is still impractical. The benchmark examples and programs are < 100
> lines of code, usually substantially less than 100 lines. I expected
> that “compositional” analysis will be able to handle larger, more
> complex programs.

We addressed this comment in our first response in the point
'Analysis for large programs'.

In short, our analysis scales well to large programs, but in practice
it is hard to find C programs with loops whose termination only relies
on integer operations.  While most loops fall in this category in our
benchmarks, one often finds loops on strings, which require a heap
invariant to prove termination, or loops on data streams (files,
sockets, ...) which also require complex invariants.  Such loops could
be handled using heuristics or program transformations that make
explicit the size of the data iterated over by the loop.

> Page 4 examples. I am still puzzled as to how the bound invariants,
> with these non-trivial constant factors, are arrived at. You claim you
> don’t need tight loop invariants but can compute tight bound invariants
> and constant factors?

The strength of encoding our problem into LP solving is that only minimal
local knowledge on the value of variables is required to get good bounds.
The tight constant factors are computed by the LP solver, not by our
abstract interpretation pass, those are two very distinct passes.

On the simple example:

    while (x>2) x -= 3;

Our abstract interpretation pass only needs to infer that 'x >= 3' inside
the loop.  Then, the constraints generated allow the LP solver to take 3
potential units from |[0,x]| inside the loop.  Because the LP solver
minimizes the linear coefficients, it assigns the potential 1/3 to the
above interval.  And 1/3 * |[0,x]| is a tight bound.

From a bird's eye, our abstract-interpretation-based invariant generator
finds local knowledge that is encoded in a global constraint set, which
is in turn optimized by the LP solver.  This interaction of local and global
information is a strength of our analysis.

> Page 6. What is the interpretation of the handling of nonlinear
> operations?

We answered this in the second point of the response to Review #41D.

> Page 6 bottom. Why there is no discussion of rule Q:Loop in the text? I
> think it will be really helpful.

We would integrate a description in the final version.  Meanwhile, a
description of the equivalent rule L:Loop can be found in our extended
version.

> Why there is no comparison with other tools on cBench?

The only tool that would readily work on the cBench code is Loopus.
By design, Loopus is not as compositional as C4B and this makes a
meaningful comparison difficult.

For example, for the sha_update function the tools compute the
following bounds:

    C4B:    2 + 3.55 |[0, count]|
    Loopus: max(0, count - 63)

The bound of Loopus is not accounting for the cost of function calls
in sha_update. Thus it does not reflect the actual cost of a call to
sha_update but rather computes a local bound.  (In this particular
example, if count <= 63, a memcpy of count bytes is run by sha_update
and this does not appear in Loopus' bound.)

Review #41F:

> That said, I think the presentation can be simplified (and if not,
> a small paragraph explaining why the below is too simplistic would
> improve the paper by justifying the complexity.)
>
> Essentially, the "potential" is an extra "state" variable that:
>
> 1. is decremented by some cost at various given operations,
> 2. must be non-negative at all operations.
>
> So if one was interested in running time, and say each operation took
> 1 unit of time, then implicitly there is a "global" that is
> decremented by one at each operation, and is required to be greater
> than 0 at all times. With this, the entire business of compositional
> analysis boils down to computing invariants that relate the program
> variables with this "global"; and compositional analysis boils down to
> computing how the value of this global "changes" across a procedure
> call (what is its value at the end -- in the postcondition vs. at the
> beginning -- in the precondition), and plain old program logics give
> you compositionality out of the box.

One has to distinguish between two different concepts of ghost state:
(1) a global ghost variable that tracks that amount of consumed
resources and (2) a global variable that corresponds to the potential
that is used as a mechanism to derive a bound on the cost.

We might well use plain old program logics to reason about the
potential using a global potential variable Phi and replace all our
Hoare triples {P} S {Q} with {Phi = P} S {Phi = Q}. But this would
only be a change in notation that would not effect the presentation.

While we understand the relation of a resource counter (1) to
invariant generation, the connection of the potential Phi (2) with
invariant generation seems much less clear.

> Then the specific tehcniques here can be viewed as using LP to compute
> relational invariants (in the abstract interpretation sense of the word)
> between the usual program parameters and this global "ghost" variable.

Such relational invariants are certainly present in the two symbolic
values for the pre- and post-potential for a program that we compute
with linear programming. However, the potential expresses not only
this invariant but also the size-changes of program variables. If one
would also express these size changes with relational invariants, one
could probably arrive at a mathematically equivalent notion. However,
it would be less intuitive to combine such relational invariants
during program composition.

> Viewed in this light the research on computing linear invariants and ranking
> functions (Colon, Sipma, Manna, Sankaranarayanan, Rybalchenko etc. etc.)
> are very closely related with the techniques here.

It true that our technique is related to the computation of linear
invariants and ranking functions. We would discuss the connection with
the mentioned works in greater detail in the final version of our
paper.  We have already mentioned relation to ranking functions on
page 3 where we state that "the concept of potential function is a
generalization of the concept of ranking function" and proceed to
explain it.
